# SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents

摘要：近年来，基于大型语言模型（LLM）的智能体在复杂推理和工具使用方面展现出了令人瞩目的能力，它们能够通过与环境的多步交互来完成相关任务。尽管这些智能体有潜力处理复杂任务，**但其问题解决过程 —— 即智能体完成任务所经历的交互轨迹 —— 尚未得到充分利用。**这些轨迹包含丰富的反馈信息，能够引导智能体朝着正确的方向去准确解决问题。

虽然蒙特卡洛树搜索（MCTS）等主流方法能有效平衡探索与利用，但它们忽略了不同轨迹之间的相互依赖关系，且搜索空间缺乏多样性，这会导致推理过程冗余以及结果欠佳。

为解决这些挑战，我们提出了 SE - Agent，这是一种能够**让智能体迭代优化自身推理过程的自我进化框架。**我们的方法通过**修正、重组和细化**这三个关键操作，对先前的试点轨迹进行重新审视和增强。这种进化机制具有两个关键优势：（1）在先前轨迹的引导下，智能体能够智能地探索多样化的解决方案路径，从而将搜索空间扩展到局部最优之外；（2）利用跨轨迹的启发，有效提升性能，同时减轻次优推理路径带来的影响。

通过这些机制，SE - Agent 实现了持续的自我进化，能够逐步提高推理质量。我们在 SWE - bench Verified 数据集上对 SE - Agent 进行了评估，以解决现实世界中的 GitHub 问题。在五种性能强大的大型语言模型上的实验结果表明，集成 SE - Agent 后，性能相对提升高达 55%，在 SWE - bench Verified 数据集上的所有开源智能体中达到了最先进的水平。

我们的代码和演示材料可在[https://github.com/JARVIS](https://github.com/JARVIS)- Xs/SE - Agent 公开获取。

---

大型语言模型（LLMs）在多个领域展现出了卓越的能力，从自然语言理解到代码生成 [1] 均有涉猎。当这些模型配备外部工具 [2; 3; 4; 5] 和环境交互能力后，便进化为能够处理日益复杂的现实世界任务的自主智能体。

然而，完成复杂任务很少能一步到位 [6; 7]。在实际应用中，大多数基于 LLM 的智能体采用与环境多轮交互的方式，遵循如 ReAct [8] 之类的框架，通过迭代来收集信息、对当前状态进行推理并采取行动。这些交互过程自然形成了轨迹 —— 即一系列状态和动作，其中编码了有价值的问题解决模式和策略 [9; 10]。每条轨迹都代表着一次完整的问题解决尝试，**不仅包含最终的解决方案，还涵盖了推理路径、环境反馈以及导致该结果的决策过程** [11; 12; 13]。

尽管这些交互轨迹中包含丰富的信息，但当前多智能体推理的方法仍存在根本性局限 [14; 15]。虽然蒙特卡洛树搜索（MCTS）等方法能有效平衡探索与利用 [16; 17]，但它们将轨迹视为独立实体，忽略了不同解决方案路径之间丰富的相互依赖关系和潜在协同作用 [18]。此外，即便采用多样化的采样策略（例如，改变温度参数或提示词），智能体往往还是会收敛到结构相似的轨迹上，这些轨迹仅在表面表达上有所不同，从而导致一个关键现象：尽管生成了多条轨迹，但最终结果却出奇地相似 [19; 20; 21]。这一局限源于概率语言模型的固有特性，它们自然倾向于高概率的解决方案模式，**进而限制了搜索空间的多样性** [22; 23; 24]。

为克服这些局限，我们提出了 SE-Agent，这是一种自我进化框架，能够让智能体通过系统性的轨迹操作来迭代优化其推理过程。我们的核心见解是，**通过在轨迹层面主动干预 —— 而非仅仅调整采样参数 —— 可以引导智能体探索本质上不同的视角和解决方案。**通过修正、重组和细化这三个核心操作，SE-Agent 不仅能生成真正多样化的轨迹，还能产生相应的多样化结果，显著扩大候选解决方案的空间。** 这种轨迹层面的干预使智能体能够发现传统采样方法可能无法涌现的新问题解决能力，有效让基础模型突破其初始性能边界。**通过战略性地整合来自多条轨迹的见解，**我们的框架提高了找到难题正确解决方案的可能性，而这些难题使用传统的多采样方法可能无法解决。我们的贡献总结如下：

* 我们引入了一种新颖的自我进化框架，该框架在轨迹层面运作，以增强智能体的推理能力。重要的是，只要复杂任务仍然需要多步推理（在可预见的未来，这一要求可能会持续存在），无论基础模型能力如何提升，我们的方法都依然有效。通过操作轨迹而非依赖采样变化，我们实现了解决方案路径和最终结果的真正多样性。
* 我们在 SWE-bench Verified [25] 上进行了全面实验，该基准是代码相关任务中最具挑战性且被广泛采用的基准之一。我们的结果表明，在不同的 LLM 上，性能都有显著提升，验证了轨迹层面的自我进化在现实世界软件工程场景中的有效性。

---

相关工作：

**Code Agents: **

代码智能体是一类专门的人工智能系统，旨在自主理解、生成和操作源代码。这类智能体已逐步发展到能够在大规模代码库中处理日益复杂的软件工程任务。在给定仓库级目标的情况下，它们会先识别相关文件和代码片段，再实施必要的修改。

本研究聚焦于 SWE - bench 任务，该任务涉及通过自动应用功能性漏洞修复来解决现实世界中的 GitHub 问题。文献 [26] 通过 SWE - agent 引入了智能体 - 计算机接口的概念，而 OpenDevin [27] 则呈现了一系列社区驱动的智能体，其中包括 CodeAct [28]。无智能体（Agentless）方法 [29] 采用简洁的定位与修复两步流程，取得了具有竞争力的性能。AutoCodeRover [30] 整合了先进的代码分析技术，包括抽象语法树和基于频谱的故障定位。阿里巴巴的 Lingma Agent [31] 提出了一种基于搜索的仓库探索策略，随后进行结构化编辑。此外，多项研究 [32; 33; 34; 35] 表明，即使在相同的智能体配置下，重复的轨迹采样也可能导致结果出现显著差异。

最近，SWE - search [36] 提出了一种多智能体框架，将蒙特卡洛树搜索（MCTS）与自我改进机制相结合，以提升在这类任务上的性能。

**智能体能力增强方面，近期研究已开发出多种提升智能体性能的方法:**

像 GoalAct [37] 这类规划框架引入了带有**分层执行的全局规划**，在 LegalAgentBench [38] 上降低了任务复杂度，适应性提升了 12.22%。在代码生成领域，RGD 框架 [39] 利用**多智能体调试进行迭代优化**，在 HumanEval 数据集上性能优于最先进方法 9.8%，在 MBPP 数据集上则高出 16.2%。

协作式方法（如 Collaborative Voyager [40]）让智能体能够相互交流、彼此学习，在有效解决幻觉问题的同时，提升了任务完成度。通过 MPO [41] 实现的元规划优化，能提供高层级指导，并基于执行反馈持续优化计划，显著提高了任务效率和泛化能力。

AutoGPT [42] 和 AgentGPT [43] 等智能体增强方法整合了工具使用功能，以此扩展智能体的能力范围；而 MemGPT [44] 和 ReAct [8] 等检索增强框架，则通过记忆机制增强了上下文理解能力。

反思（Reflexion [10]）和评论（CRITIC [45]）等自我改进技术，让智能体能够通过自我批判来迭代完善自身推理过程。尽管这些方法都展现出了潜力，但我们的研究在 ReAct 范式内引入了一种新颖方法，**它在关键步骤融入了战略性反思和变异，通过整合多条轨迹来生成优化的执行路径，且无需像测试时缩放（TTS）技术那样耗费额外的计算时间。**

---

### 面向任务的推理环境

我们考虑一类需要多步推理和执行的复杂任务。这类任务涵盖范围广泛，包括软件工程问题、数学解题、战略规划以及创意内容生成等。从形式上，我们将推理环境建模为一个元组 **E**=**(**T**,**S**,**A**,**P**,**R**)**，其中：

* **T** 代表所有需要多步推理的可能任务的集合；
* **S** 表示状态空间，每个状态 **s**∈**S** 都反映了任务解决的当前进展；
* **A** 是智能体可执行的动作空间，可能包括推理步骤、信息收集或直接执行任务等；
* **P**:**S**×**A**→**S** 定义了状态转移动态，即将状态 - 动作对映射到新状态；
* **R**:**S**×**T**→**R** 是奖励函数，用于评估特定任务下某个状态的质量。

### 推理轨迹

推理轨迹的概念是 SE-Agent 的核心，它描述了智能体在解决任务过程中，状态和动作的序列进展。给定一个任务 **t**∈**T**，推理轨迹 **τ** 被定义为一个有序序列：**τ**=**(**s**0****,**a**0****,**s**1****,**a**1****,**…**,**s**n****)**，其中：

* **s**0**** 是初始状态；
* **a**i**** 是在第 **i** 步采取的动作；
* **s**n**** 是最终状态。

每个中间状态由转移函数决定：**s**i+1=**P**(**s**i,**a**i)。轨迹 **τ** 是通过反复应用策略 **π**:**S**×**T**→**A** 生成的，该策略将当前状态和任务映射到相应的动作。策略可以包含多种推理策略，包括分解、规划和验证等。轨迹的质量由最终奖励衡量：**R**(**τ**,**t**)**=**R**(**s**n,**t**)**，它评估最终状态 **s**n 满足任务 **t** 要求的程度。

### 智能体的目标

我们的研究目标是**开发一种能够为复杂任务生成高质量推理轨迹的智能体。**具体而言，给定一个任务 **t**∈**T**，我们旨在找到一个策略 **π**∗，使期望奖励最大化：**π**∗**=**arg**max**π****E**t**∼**T****[**R**(**τ**π**(**t**)**,**t**)]**，其中 **τ**π**(**t**)** 表示策略 **π** 为任务 **t** 生成的轨迹。

---

**SE-Agent:**

在本节中，我们将介绍 SE - Agent，这是一种新颖的**自我进化范式**，旨在处理涉及多步执行的复杂任务。为了生成高奖励的轨迹，SE - Agent 会交替生成一系列经过改进的轨迹，并从中选择最优的一条。具体而言，我们**利用预先收集的试点轨迹设计了轨迹进化机制，这些试点轨迹为 SE - Agent 的模仿学习提供了参考。**

我们的主要灵感是**将试点轨迹构建为应用于学徒轨迹的 “改进算子”，这使得智能体能够通过迭代细化和跨轨迹学习，不断进化自身的推理路径。**

SE-Agent 的核心理念在于利用多个推理轨迹中蕴含的集体智慧，突破单一推理尝试的局限性。如图 1 所示，SE-Agent 通过一个进化框架运作，该框架能在迭代过程中系统性地提升轨迹质量。

给定一个任务 T，我们首先通过多维度规划与探索，生成一个多样化的初始轨迹池 T₀ = {t₁, t₂, ..., tₙ}。每条轨迹 tᵢ都代表智能体为解决任务 T 所采取的一系列推理步骤和动作。与传统方法从初始池中选择最佳轨迹后便终止的做法不同，SE-Agent 采用迭代进化过程来推导出不断优化的解决方案。

我们的 SE-Agent 重复执行以下三个基本操作：

* **修正（Revision）**：通过自我反思和针对性改进来增强单个轨迹
* **重组（Recombination）**：结合现有路径的优势创建新轨迹
* **细化（Refinement）**：通过消除冗余和提升效率来优化轨迹

每一轮迭代 i 都会生成新一代轨迹 Tᵢ，解决方案的质量逐步提升（详见图 6 中关于 Astropy 真实漏洞修复的详细案例研究，该案例展示了这一过程）。此过程持续进行，直至满足收敛标准或达到预定的迭代次数。最终输出是奖励值最高的轨迹 t∗，它能最有效地解决原始任务。

SE-Agent 的关键创新在于，它能够在先前经验的引导下智能探索解决方案空间，从而摆脱局部最优；同时，它还能利用跨轨迹的启发来高效提升性能。这种双重机制使智能体的推理能力实现了持续的自我进化。

可以将 SE-Agent 理解为一种专为大型语言模型推理设计的特殊遗传算法。从这一角度来看，我们的方法与进化计算框架在概念上有相似之处：在进化计算中，轨迹生成相当于基因型，而问题解决性能则代表表型表达。然而，与通常需要大量迭代才能收敛到可接受解决方案的传统遗传算法不同，SE-Agent 旨在通过显著更少的进化周期获得高质量结果。这种高效性源于我们的针对性操作 —— 这些操作结合了大型语言模型固有的推理能力与结构化的进化机制。

此外，SE-Agent 与强化学习中的自我对弈和专家迭代方法也有相似之处：在这些方法中，每个轨迹细化步骤都充当改进算子，通过优化探索与利用的平衡来指导后续推理。两者的关键区别在于，我们明确地对完整的推理轨迹进行操作，而非孤立的状态 - 动作对，这使得整个问题解决过程能得到更全面的改进。

### 4.2 修正操作

修正操作是 SE-Agent 自我进化能力的基础，其核心是通过内省和针对性增强来生成并改进单个轨迹。

### 4.2.1 初始轨迹生成

为了建立多样化的进化起点，我们采用了两种互补的方法。

**多规划探索：**我们首先通过改变规划策略、提示技术和推理方法，为任务 T 生成若干条不同的轨迹。这最大限度地提高了初始轨迹池的维度多样性，确保对解空间的广泛覆盖。每条轨迹 ti∈T₀的生成方式为：
ti = Plan (T, θi)
**其中，θi 代表不同的规划参数和策略。**

**基于变异的多样化**：我们通过**对现有轨迹进行受控变异，进一步扩展轨迹池，生成 M 条额外路径。这些变异在推理步骤、动作选择或中间结论中引入有针对性的变化**：
ti+m = Mutate (ti, δm)，n = 1,...,M
其中，δn 控制所应用变异的程度和性质。

这种双重方法形成了包含 10 条多样化轨迹的初始池 T₀ = T₀^plan ∪ T₀^mutate，作为后续进化的基础。

### 4.2.2 反思与修正

对于 T₀中的每条轨迹 ti，我们执行一个关键的反思过程，分析其优势、劣势和潜在的改进领域：
Ri = Reflect (ti, T)

基于这些反思，我们通过有针对性的改进推导出修正后的轨迹：
t'i = Revise (ti, Ri)

反思过程会识别逻辑不一致之处，并详细阐述不够完善的推理步骤。修正过程则会消除冗余或循环推理，并在有益的情况下纳入替代视角。

修正操作体现了 “规划起源与反思进化” 的原则，其中初始规划作为种子，通过结构化的自我反思和有针对性的改进不断进化。

### 4.3 重组操作

如果说修正操作用于增强单个轨迹，那么重组操作则通过跨轨迹学习促进集体进化。

#### 4.3.1 轨迹重组

我们实施三种互补的重组策略来生成更优的轨迹。

交叉：我们**识别不同路径中表现优异的轨迹片段，并将它们组合起来，创建继承多个 “父轨迹” 优势的混合轨迹**：
t\_new^cross = Crossover (ti, tj, α)
其中，α 决定交叉点和组合策略。

迁移学习：**将成功轨迹中的知识和策略系统地迁移到欠发达路径中，以增强这些路径**：
t\_new^transfer = Transfer (ti, {tj, tk,...}, β)
其中，β 控制迁移机制和知识适应过程。

重构：**基于轨迹池中的集体见解进行轨迹重构**：
t\_new^restructure = Restructure (Ti, γ)
其中，γ 通过全局轨迹分析指导重构过程。

### 4.4 细化操作

细化阶段是我们自我进化方法的高潮，重点在于**轨迹优化和基于综合评估指标的最终选择**。

#### 4.4.1 评估函数

为了有效指导进化过程并选择最优轨迹，我们设计了一个多维度奖励函数，从几个关键维度评估轨迹质量：
Reward (t, T) = α・TaskCompletion (t, T) + β・ReasoningQuality (t) + γ・Efficiency (t)

其中：

* TaskCompletion (t, T) 通过结构验证（例如，非空补丁文件、足够的代码编辑步骤、合理的轨迹长度）衡量轨迹 t 解决任务 T 的效果；
* ReasoningQuality (t) 评估推理过程的逻辑连贯性、深度和稳健性；
* Efficiency (t) 从推理步骤和资源利用角度量化计算效率。

超参数 α、β、γ 和 δ 控制每个评估维度的相对重要性，并可根据任务需求进行调整。

我们结合自动指标和专门的评估器来实现这个奖励函数，这些评估器会同时分析每条轨迹的过程和结果：
TaskCompletion (t, T) = AutoEval (t, T) + λ・ExpertEval (t, T)

其中，AutoEval 由基于规则的结构验证指标组成，而 ExpertEval 则纳入了基于 LLM 的解决方案质量评估。

### 4.4.2 选择与收敛

基于我们全面的评估函数，我们实现了一种战略性选择机制，通过平衡轨迹质量和多样性来推动进化过程： **\\(T\_{i+1} = \\text{Select}(T\_i \\cup T'\_i \\cup T^i\_{\\text{new}}, k)\\)**

其中，**k** 是需要保留的精英轨迹数量。该机制采用混合方法，根据奖励分数自动保留表现最佳的轨迹。同时，它通过计算轨迹 dissimilarity（差异性）指标，确保不同推理方法都能得到体现。

这一选择过程会迭代进行，直至完成预定义的进化周期数，或满足收敛标准（例如，当连续迭代中最大奖励的提升幅度低于阈值 **\\(\\epsilon\\)** 时）。最终输出是奖励最高的轨迹： **\\(t^\* = \\arg \\max\_{t \\in T\_f} \\text{Reward}(t, T)\\)**

其中，**\\(T\_f\\)** 是所有进化周期结束后的最终轨迹池。

这种精细化的选择与收敛方法体现了 “集体竞争与基因涌现” 的本质 —— 轨迹通过结构化进化过程既竞争又协作。通过这一机制，SE-Agent 实现了两个关键优势：（1）通过系统性地突破局部最优，探索更大的解空间；（2）利用跨轨迹启发高效提升性能，同时将次优推理路径的影响降至最低。这些优势使 SE-Agent 能够以前所未有的有效性和效率处理复杂的多步推理任务，彰显了自我进化的力量。



| 功能       | gemini-cli                                                                                                                   | 优缺点                                                                                                                                                                                                                   |
| ---------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 查看目录   | *`list_directory`: 列出指定目录下的文件和子目录。                                                                            | 优点：两种模式递归和非递归展现，并且再大项目中存在不重要项目过滤，控制整体项目数量操作。                                                                                                                                 |
| 读文件     | *`read_file`: 读取并返回指定文件的内容。* `read_many_files`：一次性读取多个文件的内容。                                      | 优点：模型上下文足够长，一次能塞进去非常多的文件，对文件结构了解非常深刻，在基于这个问答也非常不错。缺点：强适配特定模型，上下文1M左右，目前开源最大160K-200K之前，对Deepseek系列需要加上DUAL，Qwen3 Coder可以很好支持。 |
| 本地搜索   | *`search_file_content`: 在文件内容中搜索指定的正则表达式模式。* `glob`: 查找与特定模式匹配的文件路径 (例如 src/\*\*/\*.ts)。 | * 优点：支持正则查询、支持制定path下查询* 缺点：搜索东西过于杂乱、且缺乏图搜索。                                                                                                                                         |
| 编辑文件   | *`replace`: 替换文件中的需要修改的文本内容。                                                                                 | 优点：速度快缺点：工具多次调用，才能修改完成，前端不自然，如果文件本身有修改，再加上模型的修改，看起来一团乱，分不清谁改的。                                                                                             |
| 写文件     | *`write_file`: 将内容写入指定文件。                                                                                          | 无                                                                                                                                                                                                                       |
| 联网查询   | *`web_fetch`: 从网页链接（URL）获取内容。* `google_web_search`: 使用 Google 搜索进行网页查询。                               | 无                                                                                                                                                                                                                       |
| 命令行执行 | *`run_shell_command`: 执行一个 shell 命令。                                                                                  | 优点：可以根据系统选择合适的语言                                                                                                                                                                                         |
| 记忆       | *`save_memory`: 记住一个特定的信息或事实，用于后续交互。                                                                     | 优点：记忆md文件或者很久之前历史行为，可以提高模型表现和用户使用频率                                                                                                                                                     |
