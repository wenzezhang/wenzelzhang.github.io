# Learning without training:  The implicit dynamics of in-context learning

摘要：大型语言模型（LLM）最显著的特征之一是其上下文学习能力。也就是说，在推理阶段，当新模式以提示词中的示例形式呈现时，即便这些模式在训练中从未出现过，LLM也能学习这些新模式，且无需任何额外的权重更新。这种能力背后的机制在很大程度上仍不为人知。  在本研究中，我们发现，**将自注意力层与多层感知机（MLP）进行堆叠，会使Transformer块能够根据上下文隐式修改MLP层的权重。** 我们通过理论与实验论证，这种简单机制或许正是LLM能够进行上下文学习（而非仅在训练阶段学习）的原因。具体而言，在温和的简化假设下，我们**证明了Transformer块如何将上下文隐式转化为MLP层的低秩权重更新。**

---

本研究聚焦于大型语言模型（LLM）的上下文学习能力[3,4]——即训练完全结束后，模型能从训练中未见过、但通过提示词提供给已训练系统的示例中学习。在机器学习史上，从一系列示例中提取模式的能力被理解为一个动态过程：随着示例通过优化程序被模型“消化”，模型权重会相应更新[5]。然而，在上下文学习（ICL）中，不存在直接的显式权重更新，却能解释训练后的LLM呈现出的动态特性——它们似乎能在用户提示词的“指令”下自我重组或重新配置。LLM这种神秘且极具价值的特性，促使研究者推测：当模型“处理”提示词时，推理阶段存在一种隐式的权重更新[6-11]。近期研究甚至表明，Transformer块的简化模型会隐式执行某种梯度下降优化[7,9,10]。

在本研究中，我们延续“隐式权重更新”这一直觉，但采用了相反的研究路径。我们没有沿着“抽象简化”的方向聚焦于易处理的简化模型，而是反其道而行之，提炼出我们认为注意力层中最关键的上下文属性。这一思路引导我们提出了Transformer块的泛化形式，我们称之为“上下文块（contextual block）”。我们发现，具有这种上下文属性的层与标准神经网络堆叠时，会将上下文隐式转化为堆叠神经网络第一层的权重更新。我们提供了这种隐式更新的显式公式，结果表明其为一个秩1矩阵。这意味着，**像自注意力层这样的上下文层，在与神经网络结合后，会对MLP的权重执行某种隐式微调，而这种更新正是基于上下文计算得出的。**

主要贡献：

- 提出“上下文块”的概念，其由上下文层与神经网络堆叠而成，是Transformer块的泛化形式；

- 证明对于上下文块，在存在上下文时的token输出，与无上下文但神经网络权重矩阵被低秩矩阵更新后的token输出一致；

- 提供了与上下文效应对应的神经网络隐式权重更新的显式公式；

- 证明token的处理过程，对应于神经网络权重上的隐式梯度下降学习动态。

---


当语言模型达到一定规模后，便能从提示词中给出的示例中学习。这一点从 GPT-3 开始就已明确 [3]。这种涌现能力被称为 “上下文学习（ICL）”[4]。在文献 [12] 中，作者提出了一个核心问题：ICL 中是否真的存在推理时的 “真正学习”，还是说上下文里的示例只是帮助模型调取预训练阶段已习得的能力，而推理时并未发生真正的新知识学习？事实上，文献 [13] 认为，提示词中的示例仅起到一种贝叶斯条件作用，而非真正的学习。同样地，文献 [14] 表明，将提示词中的示例标签替换为随机标签并不会显著降低 ICL 的性能，这支持了 “示例是用于调取预训练能力而非推理时学习” 的观点。然而，对这些观点的重新探讨 [15] 显示：尽管这一点对小模型成立，但大模型确实会从提示词中随机替换的标签中学习。类似地，文献 [16] 指出，在大型语言模型中，真正 ICL 的涌现似乎还依赖于预训练数据的多样性。

从另一个规模角度，文献 [8] 表明，在回归任务上预训练的 Transformer，能在推理时通过上下文学习多种新函数，如线性函数、决策树、两层神经网络等。这些实验提供了一个可控场景，可有效测试真正的 ICL，且表明 Transformer 能学习一种文献 [17] 所描述的 “元优化器”。文献 [6] 对这一假设进行了验证：在相同的回归任务可控场景中，经梯度流训练的线性注意力 Transformer 会收敛为一种元优化器，表现得类似梯度下降。与此同时，文献 [7]、[9] 和 [10] 揭示了理论机制：推理时通过提示词处理示例的过程，可通过隐式权重更新，等价于基于最小二乘损失的梯度下降步骤。近期，文献 [11] 在相同场景中表明，思维链提示能产生多步梯度下降更新的效果。然而，所有这些隐式学习动态都仅在 “线性层” 和 “由回归示例对构成的提示词” 这些严格假设下成立。文献 [18]、[19] 对这些假设提出了批评，认为其不符合实际情况，并指出 ICL 与基于提示词示例构建微调损失的真正梯度下降之间存在差异（文献 [20] 近期表明，ICL 在泛化能力上优于微调）。

在本研究中，我们延续 “ICL 通过对应某种隐式学习动态的隐式权重更新实现” 这一思路。但我们去除了 “线性注意力层” 的限制性假设，并允许使用完全通用的提示词，使理论更贴近实际应用中的 Transformer 块。相应地，这种隐式动态需被理解为发生在 MLP 层（而非 Transformer 块的自注意力层），这与文献 [7]、[9] 和 [10] 的情况不同。

---

## 上下文块

在本节中，我们提炼出 Transformer 的一些关键特性。具体而言，我们引入 “上下文层” 的概念，它是 Transformer 块中自注意力层的泛化形式。在这一设定下，“上下文块” 由上下文层与标准神经网络组合而成，是 Transformer 块概念的泛化。随后，我们将证明核心定理：**对于上下文块，上下文的作用等价于对神经网络权重进行低秩微调更新。**为简化表述，我们先在 “无跳跃连接的神经网络” 情形下阐述结论。含跳跃连接的情形与之类似但更为复杂，其完整推导见附录 A。

一个上下文块：输入C和x输出一个向量，A（C,x),比如在transform中的自注意力中，C可以是上下文token，query是当前的问题token，prompt是[C,x]他们的拼接。A(C,x)指的是x最后一个token对应的输出。$$

上下文层会生成上下文向量，我们可借此定义**层输出在有上下文与无上下文时的差值**：∆A(C) := A(C,x) − A(x)。

基于将自注意力层泛化为上下文层的这一思路，我们现在对完整 Transformer 块的概念进行泛化，从而定义 “上下文块”：

**定义 2.1** 上下文块是由上述上下文层 A 与神经网络 Mₐ构成的复合结构 Tₐ = Mₐ ◦ A；即 Mₐ(z) = fᵨ(Wz + b)，**其中 W 和 b 是第一个全连接密集层的权重，**而 fᵨ(z) 代表神经网络的其余部分。

以下定理表明：上下文块会将上下文 C 的某一部分 Y⊂C 转化为神经网络权重的隐式更新，使得权重 W 变为 W + ∆W (Y)—— **其中 Y 所包含的信息通过更新∆W (Y) 传递到了权重中。**

从某种意义上说，上下文层通过向神经网络权重 W 隐式添加一个低秩权重更新∆W (Y)，为上下文部分 Y “加载” 了适配的网络权重。也就是说，**带有完整上下文 C 的上下文块输出，与移除了 Y 的上下文 C\\Y₁对应的上下文块输出（但 Y 已通过更新∆W (Y) 纳入权重中）是一致的。**

**定理2.2** 考虑由上下文层\$A\$与带权重矩阵\$W\$的全连接层$M_W$构成的上下文块$T_W = M_W \circ A$。给定上下文$C$和输入$x$，上下文$C$中某部分$Y \subset C$对上下文块输出的影响，等价于对$M_W$第一层权重进行一个秩1更新$W + \Delta W(Y)$。即：

$T_W(C,x) = T_{W+\Delta W(Y)}(C\setminus Y,x)$

其中，

$\Delta W(Y) = \frac{(W\Delta A(Y)) \, A(C\setminus Y,x)^T}{|A(C\setminus Y,x)|^2}$

$Delta A(Y) = A(C,x) - A(C\setminus Y,x)$是与$Y$相关联的上下文向量。需注意，$\Delta W(Y)$是秩1矩阵，因为$W\Delta A(Y)$是列向量，而$A(C\setminus Y,x)^T$是行向量。

**注 2.3** 我们的定理表明，任何上下文层都会实现从提示词到神经网络第一层的隐式权重迁移，从而隐式改变预训练神经网络的行为。在所有可能的上下文层中（例如自注意力层、循环神经网络层，或 [21] 中具有局部注意力的循环层），某些层可能比其他层更擅长提供有用的权重修改。从定理给出的隐式权重更新的特定形式，以及上下文层所决定的**A**的特殊结构出发，评估上下文层的生成能力可能是一个有趣的方向。

---

## ICL的潜在学习动态：
