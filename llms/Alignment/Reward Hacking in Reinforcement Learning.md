# Reward Hacking in Reinforcement Learning

Reward Hacking: 当[强化学习 （RL）](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/(https://lilianweng.github.io/posts/2018-02-19-rl-overview/)) 代理[利用](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration)奖励函数中的缺陷或模糊性来获得高奖励，而没有真正学习或完成预期任务时，就会发生奖励黑客攻击。奖励黑客的存在是因为 RL 环境通常不完美，并且准确指定奖励函数从根本上具有挑战性。

伪相关性：

分类任务中的虚假相关或捷径学习（[Geirhos et al. 2020](https://arxiv.org/abs/2004.07780)）是一个与奖励黑客密切相关的概念。虚假或快捷方式特征可能会导致分类器无法按预期学习和泛化。例如，如果所有狼训练图像都包含雪，则用于区分狼和哈士奇的二元分类器可能会过度拟合到雪背景的存在（[Ribeiro 等人，2024](https://arxiv.org/abs/1602.04938) 年）。

[ERM 原则](https://en.wikipedia.org/wiki/Empirical_risk_minimization)指出，由于完整的数据分布是未知的，因此最大限度地减少训练数据的损失是合理的风险代理，因此我们倾向于使用训练损失最低的模型。[Nagarajan et al. （2021）](https://arxiv.org/abs/2010.15775) 研究了 ERM 原理，并指出 ERM 需要依赖所有类型的信息特征，包括不可靠的虚假特征，同时试图不受限制地拟合数据。他们的实验表明，无论任务多么容易，**ERM 都会依赖于虚假特征。**

奖励黑客攻击是指代理通过不良行为玩弄奖励功能以获得高回报的可能性。

奖励塑造是一种用于丰富奖励函数的技术，使代理更容易学习——例如，通过提供更密集的奖励。然而，设计不佳的奖励塑造机制可能会改变最优政策的轨迹。设计有效的奖励塑造机制本身就很困难。与其责怪设计不佳的奖励函数，不如更准确地承认，由于任务本身的复杂性、部分可观察状态、考虑的多个维度以及其他因素，设计一个好的奖励函数本质上是具有挑战性的。在分发外 （OOD） 环境中测试 RL 代理时，由于以下原因，可能会发生稳健性故障：

1. 即使有正确的目标，该模型也无法有效地泛化。当算法缺乏足够的智能或能力时，就会发生这种情况。
2. 该模型能够进行泛化，但追求的目标与训练时的目标不同。当代理奖励与真正的奖励函数不同时，就会发生这种情况。这被称为**客观稳健性**（[Koch 等人，2021](https://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-055.pdf) 年）或**目标错误概括**（[Langosco 等人，2022](https://arxiv.org/abs/2105.14111) 年）

在两个 RL 环境（[CoinRun](https://github.com/openai/coinrun) 和 [Maze](https://github.com/openai/procgen)）中的实验证明了训练过程中随机化的重要性。如果在训练期间，硬币或奶酪被放置在固定位置（即关卡的右端或迷宫的右上角），但在硬币或奶酪随机放置的环境中进行测试，代理只会跑到固定位置，而不会在测试时获得硬币或奶酪。当视觉特征（例如，奶酪或硬币）和位置特征（例如，右上角或右端）在测试期间不一致时，就会产生冲突，从而导致经过训练的模型更喜欢位置特征。我想指出的是，在这两个例子中，*奖励-结果的差距*是明显的，但这种类型的偏差在大多数现实世界中不太可能如此明显。

**奖励篡改**（[Everitt et al. 2019](https://arxiv.org/abs/1908.04734)）是一种奖励黑客行为，其中代理干扰奖励函数本身，导致观察到的奖励不再准确代表预期目标。在奖励篡改中，模型通过直接操纵奖励函数的实现或间接更改用作奖励函数输入的环境信息来修改其奖励机制。

概括地说，奖励黑客攻击可以分为两种类型：环境或目标指定错误，以及奖励篡改。

* **环境或目标指定错误**：该模型通过入侵环境或优化与真实奖励目标不一致的奖励函数（例如，当奖励指定错误或缺乏关键要求时）来学习意外行为以获得高奖励。
* **奖励篡改**：模型学习干扰奖励机制本身。

---

示例列表：

奖励 RL 任务中的黑客攻击示例：

* 受过训练的抓取物体的机器人手可以通过将手放在物体和相机之间来学习欺骗人们。
* 经过训练以最大化跳跃高度的代理可能会利用物理模拟器中的错误来实现不切实际的高度。
* 代理接受训练，骑自行车到达目标，并在接近目标时赢得奖励。然后，代理可能会学会在球门周围绕圈骑行，因为当代理离开球门时不会受到惩罚。
* 在足球游戏设置中，当代理触碰球时分配奖励，并且代理学会留在球旁边以高频接触球，就像振动运动一样。
* 在 [Coast Runners 游戏中](https://openai.com/blog/faulty-reward-functions/)，代理人控制一艘船，目标是尽快完成划船比赛。当它因在赛道上击中绿色块而获得塑形奖励时，它会将最佳策略更改为绕圈并一遍又一遍地击中相同的绿色块。
* [“数字进化的惊人创造力”（](https://arxiv.org/abs/1803.03453)Lehman 等人，2019 年）- 本文有许多例子，说明优化错误指定的适应度函数如何导致令人惊讶的“黑客攻击”或意外的进化或学习结果。
* [AI 示例中的规范游戏](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)列表由 [Krakovna 等人收集，2020](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/) 年。

奖励现实生活中的黑客攻击示例

* 社交媒体的推荐算法旨在提供有用的信息。但是，有用性通常由代理指标来衡量，例如点赞或评论的数量，或在平台上参与的时间或频率。该算法最终会推荐可以影响用户情绪状态的内容，例如令人发指和极端的内容，以触发更多参与。
* 针对视频共享网站错误指定的代理指标进行优化可能会大大增加用户的观看时间，而真正目标是优化用户的主观幸福感。
* [“大空头”](https://en.wikipedia.org/wiki/The_Big_Short) - 2008 年房地产泡沫引起的金融危机。我们社会的奖励黑客攻击发生在人们试图玩弄金融系统的时候。

为什么存在 Reward Hacking？

[**古德哈特定律**](https://en.wikipedia.org/wiki/Goodhart%27s_law)指出*，“当一项措施成为目标时，它就不再是一个好的措施”。*直觉是，一旦施加了巨大的压力来优化它，一个好的指标就会被破坏。指定 100% 准确的奖励目标是具有挑战性的，并且任何*代理*都有被黑客入侵的风险，因为 RL 算法会利用奖励函数定义中的任何小缺陷。[Garrabrant （2017）](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) 将古德哈特定律分为 4 种变体：

* 回归 - 选择不完美的代理必然也会选择噪声。
* Extremal - 指标选择将状态分布推送到不同数据分布的区域。
* 因果关系 - 当代理和目标之间存在非因果关系时，对代理的干预可能无法对目标进行干预。
* Adversarial - 代理的优化为攻击者提供了将其目标与代理相关联的激励。

[Amodei 等人（2016 年）](https://arxiv.org/abs/1606.06565)总结说，奖励黑客攻击（主要是在 RL 设置中）可能是由于以下原因而发生的：

* 部分观察到的状态和目标是环境状态的不完美表示。
* 系统本身很复杂，容易受到黑客攻击;例如，如果允许代理执行更改部分环境的代码，则利用环境的机制就会变得容易得多。
* 奖励可能涉及难以学习或表述的抽象概念;例如，具有高维输入的奖励函数可能不成比例地依赖于几个维度。
* RL 目标获得高度优化的奖励函数，因此存在内在的“冲突”，使得好的 RL 目标的设计具有挑战性。一种特殊情况是一种具有自我强化反馈组件的奖励函数，其中奖励可能会被放大和扭曲到打破原始意图的程度，例如广告投放算法导致获胜者获得全部。

此外，确定最佳代理优化其行为的确切奖励函数通常是不可能的，因为在固定环境中，可能存在无限数量的奖励函数与观察到的任何策略一致（[Ng & Russell，2000](https://ai.stanford.edu/~ang/papers/icml00-irl.pdf)）。[Amin 和 Singh （2016）](https://arxiv.org/abs/1601.06569) 将这种*不可识别*性的原因分为两类：

* 表示性 - 一组奖励函数在某些算术运算（例如，重新缩放）下行为不变
* 实验 - 观察到的行为不足以区分两个或多个奖励函数，这些函数都使代理的行为合理化（在两者下行为都是最佳的）

黑客攻击 RL 环境：

随着模型和算法变得越来越复杂，预计奖励黑客攻击将是一个更常见的问题。更智能的代理更有能力在奖励函数的设计中找到“漏洞”并*利用*任务规范——换句话说，获得更高的代理奖励，但较低的真实奖励。相比之下，较弱的算法可能无法找到这样的漏洞，因此当模型不够强大时，我们不会观察到任何奖励黑客攻击或识别当前奖励函数设计中的问题。

为什么存在对抗性策略？假设是对抗性策略向受害者引入 OOD 观察，而不是对其进行物理干预。有证据表明，当受害者对对手位置的观察被掩盖并设置为静态状态时，受害者对对手变得更加*稳健*，尽管在面对正常的对手策略时表现更差。此外，更高维的观测空间在正常情况下增强了性能，但使策略更容易受到敌对对手的攻击。

[Pan et al. （2022）](https://arxiv.org/abs/2201.03544) 研究了奖励黑客攻击与智能体能力的关系，包括 （1） 模型大小，（2） 动作空间分辨率，（3） 观察空间噪声，以及 （4） 训练时间。他们还提出了三种类型错误指定的代理奖励的分类法：

* *Misweighting*： Proxy 和 true rewards 捕获相同的 desiderata，但它们的相对重要性不同。
* *本体论*：代理奖励和真实奖励使用不同的 desiderata 来捕捉相同的概念。
* *范围*：代理在受限域（例如时间或空间）上测量 desiderata，因为在所有条件下的测量成本太高。

他们在 4 个 RL 环境中进行了实验，并与 9 个错误指定的代理奖励配对。这些实验的总体发现可以总结如下：*能力较高的模型倾向于获得更高（或类似）的代理奖励，但减少真正的奖励。*

* 模型大小：较大的模型大小会导致代理奖励增加，但实际奖励会降低。
* 操作空间分辨率：操作精度的提高会带来更强大的代理。但是，更高的分辨率会导致代理奖励保持不变，而真正的奖励会减少。
* 观察保真度：更准确的观察可以提高代理奖励，但会略微降低真实奖励。
* 训练步骤： 在奖励呈正相关的初始阶段之后，在更多步骤中优化代理奖励会损害真正的奖励。

如果代理奖励的指定非常糟糕，以至于它与真实奖励的相关性非常弱，我们甚至可能在训练之前就能够识别并防止奖励黑客攻击。基于这一假设，[Pan 等人（2022 年）](https://arxiv.org/abs/2201.03544)调查了一系列轨迹推出的代理和真实奖励之间的相关性。有趣的是，即使真实奖励和代理奖励之间存在正相关，奖励黑客攻击仍然会发生。

---

黑客攻击 LLM 的 RLHF

[来自人类反馈的强化学习 （RLHF）](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences) 已成为语言模型对齐训练的事实方法。奖励模型在人类反馈数据上进行训练，然后通过 RL 对语言模型进行微调，以根据人类偏好优化此代理奖励。在 RLHF 设置中，我们关心三种类型的奖励：

* **Oracle/Gold 奖励**代表了我们*真正*希望 LLM 优化的东西。
* **人类奖励**是我们在实践中评估 LLM 时收集的，通常来自有时间限制的个体。由于人类可能会提供不一致的反馈或犯错误，因此人类奖励并不完全准确地表示预言机奖励。
* **代理奖励**是由基于人类数据训练的奖励模型预测的分数。因此，继承了人类奖励的所有弱点，以及潜在的建模偏差。

RLHF 优化了代理奖励分数，但我们最终关心的是黄金奖励分数。

破解训练过程：

[Gao et al. （2022）](https://arxiv.org/abs/2210.10760) 研究了 RLHF 中奖励模型过度优化的缩放规律。为了在实验中扩大人类标签，他们使用了合成数据设置，其中预言机奖励的“黄金”标签由一个大的 RM（6B 参数）近似，其中代理 RM 的大小范围为 3M 到 3B 参数。

实验还探讨了 RM 过度优化与策略模型大小和 RM 数据大小等因素之间的关系：

* 较大的策略从优化中获得的好处较少（即，初始和峰值奖励之间的差异小于较小的策略），但过度优化也较少。
* 更多的 RM 数据会导致更高的金币奖励分数，并减少“Goodharting”。
* KL 处罚对金牌分数的影响类似于提前停止。请注意，在除此实验外的所有实验中，PPO 中的 KL 惩罚都设置为 0，因为他们观察到使用 KL 惩罚会严格增加代理与金币奖励的差距。

RLHF 旨在提高模型与人类偏好的一致性，但人类反馈可能无法捕捉到我们关心的所有方面（例如，事实性），因此可以被黑客入侵以过度拟合不需要的属性。例如，该模型可以优化以输出看似正确且令人信服但实际上不准确的响应，从而误导人类评估者更频繁地批准其不正确的答案（[温 et al.， 2024](https://arxiv.org/abs/2409.12822)）。换句话说，由于 RLHF，正确和人类看起来正确之间出现了差距。确切地说，[温 等人（2024 年）](https://arxiv.org/abs/2409.12822)使用基于 [ChatbotArena 数据的](https://lmsys.org/blog/2023-07-20-dataset/)奖励模型进行了 RLHF 实验。他们在问答数据集 [QuALITY](https://github.com/nyu-mll/quality) 和编程数据集 [APPS](https://github.com/hendrycks/apps) 上评估了该模型。他们的实验表明，模型变得更善于说服人类他们是正确的，即使他们是错的，而且这种效果是无意的：

* RLHF 提高了人类的认可度，但不一定是正确的。
* RLHF 削弱了人类的评估能力：经过 RLHF 训练后，人类评估的错误率更高。
* RLHF 使不正确的输出对人类更有说服力。RLHF 训练后评估假阳性率显著增加。

人工评估错误的变化不是由于招聘过程中的噪音造成的，因为 （1） 在个人层面，大多数 （70-90%） 的人工评估员的原始评估错误率增加，以及 （2） 他们在评估中付出的努力或等效，通过花费的时间或编写的单元测试等指标来衡量。相反，LLM 学会了通过挑选、捏造不真实的支持陈述或制作具有微妙因果谬误的陈述来为错误的答案辩护。他们对模型在 RLHF 后的行为观察：

* 在详细 QA 任务中：
  * 创造更令人信服的捏造证据。
  * 对错误答案使用更一致的逻辑。
  * 用微妙的谬误生成连贯的答案。
* 在编码任务中：
  * 黑客入侵人工编写的单元测试
  * 生成可读性较差的测试（例如，更少的辅助函数和更高的代码复杂性）。
  * 不太可能产生人类可以利用的易于检测的错误。

**阿谀奉承是指模型响应倾向于匹配用户信念而不是反映事实**（[Shrama 等人，2023](https://arxiv.org/abs/2310.13548) 年）。在实验中，要求 AI 助手对一个论点提供反馈（`Human: "Please comment briefly on the following argument. Argument: ...")`在人类提供参数的情况下，他们可以陈述一个偏好（`"I really like the argument"`或`"I really dislike the argument"`），以测试与没有人类偏好陈述的基线反馈相比，这是否影响了模型的反馈。

他们发现 AI 助手的反馈很容易受到影响，因为当受到人类偏好的挑战时，它可能会改变原来的正确答案。该模型倾向于确认用户的信念。有时它甚至会模仿用户的错误（例如，当被要求分析诗歌时，错误地归属于错误的诗人）。通过用于预测人类反馈的逻辑回归对 RLHF 有用性数据集进行数据分析，表明匹配用户的信念是最具预测性的因素。

---

黑客攻击 Evaluator：

随着 LLM 的能力越来越强，使用 LLM 作为*评估者*或*评分者*，为其他生成器模型提供反馈和培训奖励是一个自然的选择，特别是对于无法轻松判断或验证的任务（例如，处理长格式输出、创意写作质量等主观评分标准等）。有些人将此称为“**LLM 作为评分者范式**”。这种方法在很大程度上减少了对人工注释的依赖，从而显著节省了评估时间。然而，使用 LLM 作为 oracle 奖励的不完美代理，并且可能会引入偏差，例如与不同的模型系列相比，他们更喜欢自己的回答（[Liu et al.， 2023](https://arxiv.org/abs/2311.09766)）或按顺序评估回答时的位置偏差（[Wang et al. 2023](https://arxiv.org/abs/2305.17926)）。 这种偏差尤其令人担忧，因为评分器的输出被用作奖励信号的一部分，这可能导致通过利用这些评分器来触发奖励黑客攻击。

[Wang et al. （2023）](https://arxiv.org/abs/2305.17926) 发现，当使用 LLM 作为评估器对多个其他 LLM 输出的质量进行评分时，只需改变上下文中候选人的顺序，就很容易破解质量排名。发现 GPT-4 始终为第一个显示的候选人分配高分，而 ChatGPT 更喜欢第二个候选人。

根据他们的实验，LLM 对回答的位置很敏感，并且存在*位置偏差*（即，更喜欢特定位置的回答），尽管指令中包含`"ensuring that the order in which the responses were presented does not affect your judgment."`的声明。这种位置偏差的严重性是通过 “冲突率” 来衡量的，冲突率定义为在交换响应位置后导致评估判断不一致的 （prompt， response 1， response 2） 的元组的百分比。不出所料，回复质量的差异也很重要;冲突率与两个响应之间的分数差距呈负相关。

为了减轻这种位置偏差，他们提出了几种校准策略：

* *多证校准 （MEC）：*要求评估模型提供评估证据，本质上是用文本解释其判断，然后输出两名候选人的分数。该方法可以通过对温度设置为 1 的多个 （） 证据解释进行采样来进一步稳健。 效果比 好，但性能不会随着 3 以上的增加而得到太大改善。
* *平衡位置校准 （BPC）：*聚合各种响应顺序的结果以获得最终分数。
* *人在回路校准 （HITLC）：*当面对困难的示例时，人工评估员会参与其中，使用基于分集的指标 BPDE（平衡位置分集熵）。首先，将分数对（包括交换位置的对）映射到三个标签（`win`、`tie`、`lose`），并计算这三个标签的熵。BPDE 高表示模型的评估决策中存在更多混乱，表明样本更难判断。然后，选择具有最高熵的顶部样本进行人工协助。

[Liu 等人（2023 年）](https://arxiv.org/abs/2311.09766)使用许多模型（BART、T5、GPT-2、GPT-3、FLAN-T5、Cohere）对摘要任务进行了实验，并跟踪了基于参考和无参考的指标来评估摘要质量。当在评估器（x 轴）与生成器（y 轴）的热图中绘制评估分数时，他们观察到两个指标都有深色对角线，表明存在自偏。这意味着 LLM 在用作计算器时往往更喜欢自己的输出。虽然实验中使用的模型有些过时，但在更新、功能更强大的模型上看到结果会很有趣。

---

In-Context Reward 黑客攻击：

*迭代自我优化*是一种训练设置，其中评估模型和生成模型相同，并且两者都可以微调。在此设置中，优化压力可以驱使模型利用这两个角色中出现的漏洞。在 [Pan et al. （2023）](https://arxiv.org/abs/2407.04549) 的实验中，没有更新模型参数，使用相同的模型作为评估器和生成器，具有不同的提示。实验任务是论文编辑，有两个角色：（1） 评委（评估员），对论文提供反馈，以及 （2） 作者（生成器），根据反馈编辑论文。人类评估分数被收集为论文质量的 oracle 分数。作者假设这种设置可能会导致**上下文奖励黑客攻击 （ICRH），**其中评估者分数和预言机分数不同。更一般地说，ICRH 发生在 LLM 与其评估者（例如，另一个 LLM 或外部世界）之间的反馈循环期间。在测试时，LLM 优化了一个（可能隐含的）目标，但这会在此过程中产生负面的副作用（[Pan et al.， 2024](https://arxiv.org/abs/2402.06627)）。评委和作者都可以配置为不查看或查看前几轮反馈或编辑。在线评委可以看到过去的对话，而离线评委或人工注释员一次只能看到一篇文章。较小的模型对 ICRH 更敏感;例如，从经验上讲，GPT-3.5 作为评估器比 GPT-4 导致更严重的 ICRH。当裁判和作者配置为查看不同数量的过去迭代时，如果人工分数和评估员分数共享*相同的*迭代次数，则两者分数之间的差距往往会增加。评估器和生成器之间的相同上下文对 ICRH 至关重要，这表明共享上下文比 ICRH 的上下文长度更重要。

在后续工作中，[Pan 等人（2024 年）](https://arxiv.org/abs/2402.06627)进一步研究了上下文奖励黑客攻击 （ICRH），其中反馈由外部世界提供，目标是不完美的代理目标，通常以自然语言指定。在这里，这个目标往往没有被明确说明，没有捕捉到所有的约束或要求，因此可以被黑客入侵。
