# Entropy Controllable Direct Preference Optimization

摘要：在大语言模型（LLMs）的训练后阶段，从人类反馈中进行强化学习（RLHF）是一种有效的方法，可使生成内容符合人类偏好。直接偏好优化（DPO）允许使用简单的二元交叉熵损失进行策略训练，而无需奖励模型。DPO的目标由反向KL散度进行正则化，这会促使模型寻求与参考策略的拟合模式。尽管如此，我们指出最小化反向KL散度可能无法捕捉参考分布的一种模式，这可能会损害策略的性能。基于这一观察，我们对DPO提出了一个简单的修改，即H - DPO，它可以控制所得策略的熵，提高分布的锐度，从而更有效地实现模式寻求拟合。在我们的实验中，我们表明H - DPO在各种任务上的表现优于DPO，在数学任务的pass@k评估中展示出了卓越的结果。此外，H - DPO易于实现，只需对DPO的损失计算进行微小修改，这使其具有很高的实用性，并有望在大语言模型的训练中得到广泛应用。
