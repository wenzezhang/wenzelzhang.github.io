# s1: Simple test-time scaling

摘要：

测试时扩展是语言建模领域一种颇具前景的新方法，它借助额外的测试时计算资源来提升模型性能。近期，OpenAI的o1模型展现出了这种能力，但其并未公开具体方法，这引发了众多复现尝试。我们致力于探寻实现测试时扩展以及强大推理性能的最简方法。

首先，我们精心整理了一个包含1000个问题及其推理过程的小型数据集s1K。在筛选数据时，我们依据了三个标准，这些标准通过消融实验得以验证，分别是难度、多样性和质量。

其次，我们开发了“预算强制”机制，用于控制测试时的计算量。具体而言，当模型试图结束生成过程时，我们会强制终止其思考流程；或者多次在模型的生成内容后添加“Wait”（等待）来延长其思考时间。这能促使模型对答案进行二次检查，从而修正推理过程中出现的错误步骤。

**在使用s1K数据集对Qwen2.5 - 32B - Instruct语言模型进行有监督微调，并为其配备“预算强制”机制后，我们的模型s1 - 32B在竞赛数学问题（如MATH和AIME24）上的表现比o1 - preview高出了多达27%。**此外，通过“预算强制”对s1 - 32B进行扩展，使得模型在无需测试时干预的情况下就能进一步提升性能，例如在AIME24测试中，准确率从50%提高到了57%。  我们的模型、数据和代码已在https://github.com/simplescaling/s1上开源。

---

然而，尽管有大量针对 o1 模型的复现尝试，但没有一个公开实现了清晰的测试时扩展行为。因此，我们提出疑问：**实现测试时扩展和强大推理性能的最简单方法是什么？**

s1K: consists of 1,000 carefully curated questions paired with **reasoning traces and answers distilled from Gemini Thinking Experimental**

我们针对以下两方面进行了大量的消融实验：（a）我们选取的 1000 个（1K）推理样本；（b）我们的测试时扩展方法。

对于（a），我们发现将难度、多样性和质量指标共同纳入样本选择算法至关重要。随机选择样本、选择推理过程最长的样本，或者仅选择多样性最大的样本，都会导致性能显著下降（在 AIME24 测试中平均下降约 30%）。在包含 5.9 万个示例的完整数据池（s1K 的超集）上进行训练，相比使用我们选取的 1000 个样本，并没有带来实质性的性能提升。这凸显了精心选择数据的重要性，也与此前关于指令微调的研究结果相呼应（周等人，2023 年）。

对于（b），我们为测试时扩展方法设定了评估标准，以便比较不同的方法。“预算强制”方法实现了最佳的扩展效果，因为它具有完美的可控性，并且随着计算资源的增加呈现出明显的正斜率趋势，从而带来出色的性能表现。

---

**Reasoning data curation to create s1K**

我们依据三条指导原则，从 16 个不同来源收集了最初的 59,029 个问题。

- \*\*质量\*\*：数据集应具备较高质量；我们会始终检查样本，对于格式不佳等情况的数据集直接忽略。

- \*\*难度\*\*：数据集应具有挑战性，需要进行大量的推理工作。

- \*\*多样性\*\*：数据集应来自不同领域，以涵盖不同的推理任务。

我们收集了两类数据集：

现有数据集整理

- 我们最大的数据源是 NuminaMATH（李等人，2024 年），其中包含从在线网站收集的 30,660 道数学问题。

- 我们还纳入了美国数学邀请赛（AIME）的历史真题（1983 - 2021 年）。

- 为了增强多样性，我们加入了 OlympicArena（黄等人，2024a），其中有 4,250 道涵盖天文学、生物学、化学、计算机科学、地理学、数学和物理学等多个学科奥林匹克竞赛的问题。

- OmniMath（高等人，2024a）提供了 4,238 道竞赛级别的数学问题。

- 我们还纳入了来自 AGIEval（钟等人，2023 年）的 2,385 道问题，这些问题来自诸如 SAT 和 LSAT 等标准化考试，涵盖英语、法律和逻辑等领域。其他数据源详见§B 中的表 6。

定量推理新数据集 为了补充这些现有数据集，我们创建了两个原创数据集。

- s1 - prob 包含来自斯坦福大学统计系博士资格考试概率部分（https://statistics.stanford.edu）的 182 道问题，同时附有手写解答，其中涉及一些难题的证明过程。概率资格考试每年举行一次，需要具备专业水平的数学问题解决能力。
- s1 - teasers 包含 23 道具有挑战性的脑筋急转弯，这些题目常用于量化交易岗位的面试。每个样本都包含一个问题及其解答，均来自 PuzzledQuant（https://www.puzzledquant.com/）。我们只选取了难度最高（“难”）的题目。

对于每个问题，我们使用谷歌双子座快速思考 API（谷歌，2024 年）生成推理过程和解答，并提取其推理轨迹和响应。这样就得到了 5.9 万个由问题、生成的推理轨迹和生成的解答组成的三元组。数据集中的示例详见§C.2。  我们使用 8 - 元语法（8 - grams）对所有样本进行去污染处理，以避免与我们的评估问题（MATH500、GPQA 钻石组、AIME24；§B.5）重复，并对数据进行了去重操作。

**数据集筛选：**

质量筛选

首先，我们剔除了所有调用 API 时出现错误的问题，这使得我们的数据集样本数量减少到 54,116 个。接下来，我们通过检查样本是否包含存在格式问题的字符串模式（如 ASCII 艺术图、不存在的图片引用或不一致的问题编号）来过滤掉低质量的示例，经过这一步，数据集样本数量进一步减少到 51,581 个。从这个数据集中，我们从那些我们认为质量高且无需进一步筛选的数据源中挑选出 384 个样本，作为最终 1000 个样本的一部分（具体细节见§B.4）。

难度筛选

对于难度筛选，我们使用两个指标：模型性能和推理轨迹长度。我们在每个问题上对两个模型进行评估：Qwen2.5 - 7B - Instruct 和 Qwen2.5 - 32B - Instruct（Qwen 等人，2024），并由 Claude 3.5 Sonnet 根据参考解答来判断每个模型解答的正确性（评分协议详见§B.3）。我们使用 Qwen2.5 分词器测量每个推理轨迹的 token 长度，以此作为问题难度的指标，这里基于的假设是更难的问题需要更多的思考 token。根据评分结果，我们剔除了 Qwen2.5 - 7B - Instruct 或 Qwen2.5 - 32B - Instruct 能够正确解答的问题，因为这些问题可能太简单。通过使用两个模型进行筛选，我们降低了由于某个模型在简单问题上偶然出错而导致简单样本未被过滤掉的可能性。经过这一步，样本总数减少到 24,496 个，为接下来基于多样性的下一轮子采样做好了准备。虽然使用这两个模型进行筛选可能是针对我们的实验设置进行优化的（因为我们后续也会使用 Qwen2.5 - 32B - Instruct 作为要微调的模型），但基于模型的筛选思路可以推广到其他实验设置中。

多样性筛选

为了量化多样性，我们使用 Claude 3.5 Sonnet 根据美国数学学会的数学学科分类（MSC）系统（例如，几何、动力系统、实分析等）将每个问题分类到特定的领域。这个分类体系主要聚焦于数学主题，但也涵盖了生物学、物理学和经济学等其他学科。为了从 24,496 个问题的数据集中挑选出最终的示例，我们首先从所有领域中均匀随机选择一个领域。然后，根据一个倾向于更长推理轨迹的分布从该领域中采样一个问题（具体细节见§B.4），这与难度筛选部分的思路一致。我们重复这个过程，直到选出总共 1000 个样本。  这三个阶段的筛选过程得到了一个涵盖 50 个不同领域的数据集（详见表 5）。在§5.1 中，我们将表明综合使用这三个筛选标准非常重要，因为仅依靠质量、多样性或难度中的单一标准会得到质量更差的数据集。数据集中的示例详见§C.2。
