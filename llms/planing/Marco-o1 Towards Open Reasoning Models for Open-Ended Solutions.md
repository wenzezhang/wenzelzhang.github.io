# Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions

ç‰¹ç‚¹ï¼šä¸ä»…ä»…æ˜¯å…³æ³¨æœ‰å›ºå®šç­”æ¡ˆçš„é—®é¢˜ï¼ˆæ•°å­¦ï¼Œç‰©ç†ï¼Œä»£ç ï¼‰ï¼Œä¹Ÿå…³æ³¨äºå¼€æ”¾æ€§é—®é¢˜çš„å›ç­”ã€‚

æ ¸å¿ƒé—®é¢˜ï¼šCan the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?

æ–¹æ³•ï¼šCoTå¾®è°ƒï¼Œè’™ç‰¹å¡æ´›æœç´¢ï¼Œåæ€æœºåˆ¶ï¼Œæ¨ç†ç­–ç•¥ã€‚ä½¿ç”¨Open-O1 CoT datasetï¼ŒMarco-o1 CoT datasetï¼ŒMarco-o1 Instruction datasetç­‰æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»top-kçš„å¯¹æ•°æ¦‚ç‡å¯ä»¥å¯¼å‡ºå¤šæ¡æœç´¢è·¯å¾„ï¼Œé€šè¿‡æ¢ç´¢è¿™äº›æœç´¢è·¯å¾„å¾—åˆ°æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚

* åˆæˆCoTæ•°æ®
* é€šè¿‡MCTSæ¥æ‰©å±•è§£å†³æ–¹æ¡ˆçš„spaceï¼ˆæ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼‰
* è§£é‡ŠåŠ¨ä½œç­–ç•¥å’Œåæ€æœºåˆ¶
* åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„åº”ç”¨ï¼šexploring inference-time scaling laws in the multilingual and translation domain.

æ•ˆæœï¼šåœ¨MGSM (English)æ•°æ®é›†ä¸Š+6.17%ï¼Œåœ¨MGSM (Chinese) +5.60%ï¼Œæ­¤å¤–åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºä»–å¯¹å£è¯­ç»†å¾®å·®åˆ«çš„å‡ºè‰²æŒæ¡ã€‚

---

æ•°æ®é›†ï¼š

* Open-O1 CoT Dataset (Filtered) [Team, 2024]: ç­›é€‰å’Œè¿‡æ»¤ï¼Œ45,125æ¡æ ·æœ¬
* Marco-o1 CoT Dataset (Synthetic)ï¼šä½¿ç”¨MCTSæ¥ç”ŸæˆCoTæ•°æ®é›†ï¼Œ10000æ¡æ ·æœ¬
* Marco Instruction Datasetï¼š60266æ¡æ ·æœ¬

MCTSï¼š

![image.png](assets/Marco-o1-MCTS.png)

* Nodes as Reasoning States: In the MCTS framework, each node represents a reasoning state of the problem-solving process.
* Actions as LLM Outputs: The possible actions from a node are the outputs generated by the LLM. These outputs represent potential steps **or mini-steps in the reasoning chain.**
* Rollout and Reward Calculation: During the rollout phase, the LLM continues the reasoning process to a terminal state.
* Guiding MCTS: This reward score ğ‘… is used to **evaluate and select promising paths** within the MCTS, effectively guiding the search towards more confident and reliable reasoning chains.
  * Selection: é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œa(å‡æƒ³åŠ¨ä½œ)ï¼š $score(a) = Q(a) + n * \frac{\pi(a|s_t;\theta)}{1+N(a)}$ å¦‚æœä¸€ä¸ªåŠ¨ä½œè¢«æ¢ç´¢è¿‡å¤šæ¬¡ï¼Œé‚£ä¸ªä»–çš„æ¦‚ç‡ä¼šé™ä½ã€‚
  * Expansion: å¯¹æ‰‹æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼ŒçŠ¶æ€æ›´æ–°ã€‚æ ¹æ®ç­–ç•¥ç½‘ç»œæ‰§è¡Œä¸€ä¸ªåŠ¨ä½œã€‚
  * Evaluationï¼šè¯„ä»·çŠ¶æ€ä»·å€¼å¾—åˆ°åˆ†æ•°Vï¼Œç©æ¸¸æˆç›´åˆ°ç»“æŸè·å¾—rewardï¼Œr,å°†(v+r)/2ä½œä¸ºåŠ¨ä½œaçš„scoreã€‚ï¼ˆFast RollOut:æ¨¡æ‹Ÿå¯¹å±€ç›´åˆ°ç»“æŸï¼‰
  * BackUp: ä½¿ç”¨(v+r)/2æ¥æ›´æ–°åŠ¨ä½œ-ä»·å€¼ã€‚

confidence score: åœ¨rollouté˜¶æ®µç”Ÿæˆçš„æ¯ä¸ªtokenï¼Œè®¡ç®—å¾—åˆ†ä¸ºtop5çš„tokensçš„softmaxæ¦‚ç‡å€¼ã€‚åœ¨è·å¾—åºåˆ—çš„æ‰€æœ‰tokenåï¼Œè®¡ç®—tokençš„å¹³å‡åˆ†å€¼ï¼Œä½œä¸ºreward.

---

Actionè®¾è®¡ï¼š

* Step as Action VS Mini-step as Actionï¼ˆ32æˆ–è€…64ä¸ªtokenï¼‰

Reflection after Thinkingï¼š

* **We introduced a reflection mechanism by adding the phrase â€œWait! Maybe I made some mistakes! I need to rethink from scratch.â€ at the end of each thought process. Implementing this reflection has yielded significant improvements, especially on difficult problems that the original model initially solved incorrectly. With the addition of reflection, approximately half of these challenging problems were answered correctly.**
