# FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness

摘要：目前，优化深度学习算法需要缓慢的手动推导过程，这可能导致大量性能潜力未被挖掘出来。像 FlashAttention 这样的方法，通过避免不必要的数据传输，相较于原生的 PyTorch 实现了 6 倍的性能提升，但这一成果历经了三年时间、进行了三次迭代才达成。自动化编译方法一直以来都较为滞后。图形处理器（GPU）同时受到向处理器传输数据以及可用计算能力的限制，而且传输带宽的提升速度要慢得多。目前，传输带宽已经占到了 GPU 能耗成本的 46%。这表明，高能效且节省成本的算法的未来发展依赖于对传输成本（输入输出感知，即 IO-awareness）的更完善考量，以及一种用于推导优化算法的系统化方法。

在本文中，我们提出了一种针对深度学习模型的图示化方法，通过简单的重新标记操作，就能推导出考虑到低层级内存的最优实现方式和性能模型。这些图示可沿着 GPU 层级向下推广，为比较硬件和量化选择提供了一个通用的性能模型。图示能够生成伪代码，进而揭示出特定硬件特性（如合并内存访问、张量核心操作以及重叠计算等）的应用情况。我们给出了适用于安培（Ampere）架构的注意力算法（在该架构下每个流式多处理器（SM）可容纳 13 个线程束，而 FlashAttention 只能容纳 8 个），以及适用于浩珀（Hopper）架构的注意力算法（该架构改进了重叠计算，或许能实现 1.32 千万亿次浮点运算（PFLOPs））。

---

为了执行一项运算，图形处理器（GPUs）必须将数据从高层级的动态随机存取存储器（DRAM）传输到低层级的计算核心。GPU所受的限制既来自可用计算的每秒万亿次浮点运算（TFLOPs）能力，也同样受制于以千兆字节每秒（GB/s）为单位的内存带宽。然而，人工智能模型已经遭遇了内存墙问题——随着计算能力的提升速度（每两年提升1.5倍）远比动态随机存取存储器带宽的提升速度（每两年提升1.6倍）快得多，算法越来越受到带宽/传输成本的限制。此外，动态随机存取存储器已经占到了系统总功耗的46%。随着相对于计算而言，内存的效率变得越来越低，考虑传输成本——输入输出感知的重要性将变得愈发关键。

图形处理器（GPU）的硬件特性对性能有着重大影响，且这种影响会因目标算法的不同而有所变化。在A100、H100 SXM5以及H100 PCIe（英伟达，2022年，第39页）之间做选择时，我们必须考虑到它们各不相同的计算能力、带宽、中间层级结构、架构特性以及低层级内存情况，而我们在环境和经济资源方面也会为此付出相应代价。这些特性的应用往往并非显而易见，比如在FlashAttention - 2（道，2023年）发布之时，适用于FlashAttention - 3的硬件其实就已经存在了（沙阿（Shah）等人，2024年），而FlashAttention - 3在正向传播速度方面实现了约75%的提升。了解GPU特性的影响是创新优化方法以及充分利用已部署资源的必要环节。

本文提出了一种基于神经电路图（阿博特，2023年）的用于表示深度学习算法的图示方案（见第2节），该方案可用于快速推导诸如FlashAttention之类的方法，同时还能生成一个将低层缓存大小纳入考量的传输成本性能模型（见第3节）。接着，我们展示了该性能模型如何扩展至多层级结构，给出了考虑图形处理器（GPU）层级配置以及算法对内存敏感度的表达式（见第4节）。最后，我们展示了如何将这些图示转换为伪代码，而伪代码能够揭示诸如合并内存访问、张量核心操作以及重叠操作等特定硬件特性的应用情况（见第5节）。为了展示这一方法的优势，我们提出了适用于安培（Ampere）架构和浩珀（Hopper）架构的注意力算法，相较于FlashAttention，它们降低了低层级内存的使用量。

---
