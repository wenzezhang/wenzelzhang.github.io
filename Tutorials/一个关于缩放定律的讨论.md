# 

## 缩放定律 - O1 Pro 架构、推理训练基础设施、Orion 和 Claude 3.5 Opus“失败” -AI 实验室合成数据基础设施、测试时间计算的推理代币经济学、数据墙、评估被打破、RLAIF、推理时间搜索、比以往任何时候都更需要规模

虽然很多人声称扩展定律的终结，基准测试已经饱和，较新的模型几乎没有显示出改善的迹象。批评者还指出，可用训练数据的枯竭和训练硬件扩展速度的放缓。但是尽管存在这种焦虑，大型人工智能实验室和超大规模企业加速数据中心建设和资本支出的做法不言而喻。关键决策者似乎坚定地相信扩展定律仍然存在且运行良好。为什么？

## 扩大训练，新旧模式继续存在

现实情况是，除了简单地关注预训练之外，还有更多的维度可以进行扩展，而预训练一直是大多数兼职预测者的唯一关注点。OpenAI 的 o1 版本已经证明了推理模型的实用性和潜力，为扩展开辟了一个尚未探索的新维度。然而，这并不是唯一一种随着计算规模扩大而显著提高模型性能的技术。其他通过增加计算量来改进模型的领域包括合成数据生成、近端策略优化 (PPO)、功能验证器和其他用于推理的训练基础设施。扩展的沙地仍在变化和发展，随之而来的是整个人工智能开发过程的持续加速。

从错误的基准转向更具挑战性的基准将有助于更好地衡量进展。在本报告中，我们将概述**旧的训练前扩展趋势**以及**训练后和推理时间的新扩展趋势**。这包括新方法将如何推动前沿——并且将需要比以前想象的更多的训练时间计算扩展。

将从训练基础设施和推理代币经济学的角度介绍 OpenAI o1 和 o1 Pro 的架构，包括成本、KVCache 扩展、批处理等。我们还将深入研究领先的 AI Lab 合成数据和 RL 基础设施。最后，我们想澄清 Anthropic 的 Claude 3.5 Opus 和 OpenAI 的 Orion 的“失败”，以及未来的扩展计划。

## 摩尔定律

摩尔定律的终结是半导体行业面临的另一堵墙，但最近这场争论已经平息，因为 Nvidia 等 AI 先驱通过沿几个全新维度扩展，实现了巨大的计算增益。先进封装通过扩展输入/输出 (I/O) 并使芯片能够利用超出光罩尺寸限制的总硅面积，推动了计算能力的持续进步。芯片内和芯片之间的并行计算以及[构建更大的高带宽网络域](https://semianalysis.com/2024/04/10/nvidia-blackwell-perf-tco-analysis/)使芯片能够更好地大规模协同工作，[尤其是在推理方面](https://semianalysis.com/2024/04/10/nvidia-blackwell-perf-tco-analysis/)。

就像 2004 年的计算机爱好者一样，主流分析师和记者们只见树木不见森林：尽管一种趋势正在放缓，但由于其他新兴范式已经成熟，可供扩展和扩张，整个行业仍在以惊人的速度前进。可以叠加“扩展定律”——预训练将成为改进的载体之一，而总体“扩展定律”将继续扩展，就像摩尔定律在过去 50 多年中一样。

## 扩展预训练的挑战——数据墙、容错

扩展预训练为模型性能带来了显著的提升，但也存在一些阻碍，目前业界正集中精力予以克服。一个明显的障碍是数据越来越难收集——虽然互联网上的数据正在迅速增长，但增长速度却与计算速度不成比例。这就是为什么今天的万亿参数巨型模型还没有达到 Chinchilla 最优水平——训练标记的数量与模型参数的数量相比要少得多。

Chinchilla 缩放是指数据与参数数量相对于计算量的最佳增长。数据不足会导致模型泛化能力差，而数据过多会导致过度训练，从而浪费计算资源。在某些情况下，偏离最佳比率是合理的：过度训练模型（例如 GPT-4o 和 Llama）可以显著降低推理成本，对于拥有较大用户群来为该模型提供服务的提供商而言，这是更好的选择。

2023 年 1 月，在 GPT-4 发布之前，我们撰写了有关扩展的实际限制以及 GPT-4 计划如何突破这些限制的文章。从那时起，模型就一直在从超过 Chinchilla 最优（数据量远大于模型参数）到低于 Chinchilla 最优（数据受到限制时）之间来回波动。过去，当训练和推理硬件的改进缓解了限制时，计算可用性的障碍就被克服了。

就当今关于减速带的叙述而言，有用的数据源（例如教科书和文档）已经耗尽，剩下的大多是质量较低的文本数据源。此外，网络数据仍然是分布较窄的数据，模型需要更多分布以外的数据才能继续推广。由于模型更难以最佳方式扩展，预训练变得越来越具有挑战性。

此外，如果实验室在不断扩展时训练模型的数据量不足，模型就会过度参数化，变得效率低下，并导致大量记忆而不是泛化。实验室转而越来越多地使用**合成数据**来缓解这个问题。

不过，这个问题对主要的 AI 实验室影响不大。仅 Meta 一家就拥有比公共互联网上多 100 倍的数据（如果他们能够以合规的方式利用这些数据）。这可能使他们在继续扩大规模方面具有优势，而且问题比其他公司更少。YouTube 每天上传 720,000 小时的新视频——我们认为 AI 实验室才刚刚开始考虑对视频中包含的大量数据进行训练。除此之外，他们还能够生成高质量的合成数据，我们将在后面讨论其架构。

**要对视频中可用的千万亿个替代标记**进行训练，需要继续大幅扩展整体训练 FLOP，这将通过硬件创新和系统工程来实现。例如，将训练 FLOP 再扩大一个数量级将[需要多数据中心训练](https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/)，因为所需的加速器数量已无法容纳在单个数据中心站点内。Rainier 项目让亚马逊为 Anthropic 提供了 400k Tranium 2 芯片，但以原始 FLOP 计算，这还不到 100k GB200。Anthropic 必须取得重大工程成就才能在这样的集群中进行训练。将加速器分布在大型校园或多个校园本身会面临阿姆达尔定律带来的重大挑战，尽管目前已经有[不少假设的解决方案](https://semianalysis.com/2024/09/04/multi-datacenter-training-openais/#multi-datacenter-distributed-training)来应对这一挑战。

扩展参数的另一个制约因素是推理经济学。AI 实验室可以将大量投资用于训练大型模型，并将模型的使用分摊给庞大且不断增长的用户群以及内部用例，以开发进一步的模型迭代。在推理方面，他们必须小心谨慎，不要将成本过高或不经济的模型推向市场。

评估也不全面；现有评估没有很好地涵盖模型的许多功能或属性。迁移学习（模型通过学习其他内容来提高某个领域的水平）和情境学习都是需要开发更多评估的领域。最后，总会有一些最终用例可能难以提前预测，但会为最终用户带来巨大好处。

凡是经过衡量的，便会得到改进。

## 更新、更难的评估

新的评估已经出现，旨在更好地区分模型并专注于直接解决特定的有用应用。SWE-Bench 是当今最重要的评估之一，旨在让模型解决来自开源 Python 存储库的人工审查的 GitHub 问题。新的 Claude 3.5 Sonnet 目前在 SWE-Bench Verified 上取得了 49% 的 (State of the Art) 成绩，但大多数模型的成绩要低得多。

另一个例子是研究 AI 研发能力的基准，有人将其[描述](https://x.com/_sholtodouglas/status/1860228530338152587)为“最重要的跟踪能力”。研究工程基准 (RE) 包括七个具有挑战性且开放的 ML 研究环境。人类通常在较长时间范围内的评估中表现更好，但在 2 小时的时间范围内，最好的 AI 代理获得的分数是人类的 4 倍。上述重要任务是人类目前占主导地位的，是扩展推理时间计算的完美基础。我们预计，更好地利用这种扩展形式的模型将在未来超越人类。

另一个趋势是评估包含极其困难的专家级问题。两个突出的例子是研究生级 Google 证明问答基准 (GPQA) 和 Frontier Math。GPQA 由 448 道化学、生物和物理学领域的多项选择题组成。作为背景，OpenAI 发现专家级人类（即拥有博士学位的人）在 GPQA Diamond 上的得分约为 70%，而 o1 在同一组上的得分为 78%。去年，具有搜索功能的 GPT-4（以及弃权的 CoT）[在 GPQA Diamond 上的得分为 39%](https://arxiv.org/pdf/2311.12022)。

使用极难题目趋势的另一个例子是 FrontierMath (FM)。FM 是数百道原始数学问题的基准，人类可能需要数小时甚至数天才能解答。它涵盖了广泛的数学主题，包括数论、实分析等。这项评估的特别之处在于它不会公开，从而最大限度地降低了数据污染的风险，并且可以通过自动验证器进行评分——简化了评估过程。

这项基准测试中表现最好的模型为 2%，但实验室预计这一数字将大幅提高。Anthropic 的目标是在中期内[在 FrontierMath 上达到 80% 。](https://importai.substack.com/p/import-ai-391-chinas-amazing-open?utm_source=post-email-title&publication_id=1317673&post_id=151483024&utm_campaign=email-post-title&isFreemail=true&r=e6s2j&triedRedirect=true&utm_medium=email)

## 训练后：一个新的扩展领域

预训练往往是缩放定律争论的焦点，因为它很容易理解，但它只是人工智能生命周期的一部分。模型预训练完成后，仍然需要做大量工作才能投入使用。预训练的目标非常狭义，就是“正确预测下一个标记”。即使做到了这一点，我们距离 LLM 开发的最终目标——“回答用户提示”或“完成任务”还有很大差距。

在深入探讨 OpenAI 的 O1 Pro 模型的工作原理和创建方式之前，我们将概述监督微调 (SFT)、强化学习 (RL) 和合成数据。

### 监督微调

监督式微调 (SFT) 是最著名的后训练类型。向模型展示一组精选的输入和输出对数据集，其中“演示数据”涵盖特定领域（例如代码、数学、指令遵循等）。与预训练不同，**微调数据的质量比数量更重要。**鉴于数据量较少，这意味着计算密集度较低。

GPT 的神奇之处最初在于使用来自 Scale AI 等公司的经过精心挑选的人工生成和标记数据样本。然而，随着时间的推移，人工生成的数据难以扩展。

## 合成数据在后期训练中的重要作用

SFT 中最重要的挑战是在所需领域构建足够大、高质量的数据集。这可以让模型在代码、数学、推理等特定领域更好地运行，并且**由于迁移学习，溢出效应也会使模型在其他领域也表现更好。**显然，数学和编码技能强的模型在一般推理方面更胜一筹，但这也延伸到了其他领域——**用中文和英文训练的模型在英文方面比只用英文训练的模型表现更好。**合成数据开辟了一个维度，可以使用受控、可扩展性更强的方法来生成高质量数据，从而针对任何有意愿创建它的主题对模型进行微调。

大量使用合成数据也激励人们开发更好的模型。例如，OpenAI 比其他任何人都先拥有 GPT-4，并且可以使用它来生成比其他模型提供商更好的合成数据集——直到其他提供商拥有与之匹敌的模型。开源和中国实验室的许多模型如此迅速赶上的主要原因之一**是它们都是使用 GPT-4 的合成数据进行训练的。**

底层模型在判断任务方面表现越好，用于训练的数据集就越好。这其中固有的是它们自己的缩放定律。这就是我们得到“新 Claude 3.5 Sonnet”的方式。Anthropic 完成了 Claude 3.5 Opus 的训练，它表现良好，缩放适当（忽略那些声称不是这样缩放的否认者——这是 FUD）。

然而 Anthropic 并未发布它。这是因为 Anthropic 没有公开发布，而是*使用 Claude 3.5 Opus 生成合成数据*并进行奖励建模，从而显著改进了 Claude 3.5 Sonnet，同时还使用了用户数据。推理成本没有发生太大变化，但模型的性能发生了变化。为什么要发布 3.5 Opus，从成本角度来看，与发布经过进一步后训练的 3.5 Sonnet 相比，这样做没有经济意义？

**合成数据越多，模型就越好。更好的模型提供更好的合成数据，并充当过滤或评分偏好的更好判断者。合成数据的使用本身就包含许多较小的缩放定律，这些定律共同推动着更快地开发更好的模型。**

## 合成数据示例

### 拒绝采样

大量使用合成数据的领域之一就是生成代码数据集。这通常是通过指定各种编程任务或提示作为种子并提示模型生成与这些任务相关的问题来完成的。

然后要求模型生成一组潜在解决方案。通过相应测试或可以正确执行的解决方案将附加到训练数据集中，从而有效地过滤掉质量较差的样本，这一过程称为拒绝抽样。拒绝抽样是合成数据生成过程的重要组成部分，因为它确保数据集的质量足以在监督微调 (SFT) 或强化学习 (RL) 期间发挥作用。然而，结果是许多生成的标记被丢弃——合成数据生成需要大量计算。

这种构建用于微调的合成数据集的方法已被许多大型人工智能实验室采用，并用于对 Gemini、GPT、Llama 和 Claude 进行微调。

但拒绝抽样可能比看起来更复杂。在 Llama 的案例中，如果初始响应不正确，模型会被提示修改答案，并且该模型在第二次尝试时有 20% 的时间会得到正确的答案。在合成数据有用性的另一个例子中，Meta 团队将 Python 代码翻译成 PHP，通过语法解析和执行确保质量，并将这些额外的数据输入到 SFT 数据集中以弥补公共 PHP 代码的缺乏。这有效地证明了合成数据正被用于为代表性不足的地区可靠且可预测地生成有用的数据。

### 通过模型判断

另一个趋势是使用另一个 LLM 作为评判者。Meta 使用另一个早期版本的 Llama 3 作为拒绝采样器，充当非严格可执行代码（即伪代码）的评判者，并根据代码的正确性和风格对输出进行“通过”或“失败”的评级。在某些情况下，拒绝采样是通过同时运行的多种模型对模型进行评级来完成的。虽然在网上这比人工数据便宜，但很难实现这样的自动评判合唱。 

这里需要注意的是，在所有拒绝采样方法中，无论是否使用代码，“判断”模型越好，生成的数据集的质量就越高。虽然今年 Meta 才刚刚引入这种反馈回路，但在此之前 Anthropic 和 OpenAI 已经使用了一年或两年。

### 长上下文数据集

合成数据使用的另一个例子是较长的上下文长度。模型在上下文长度上限下进行预训练（因为大多数数据的上下文长度已经很短），而且序列长度越长，内存中需要保存的 KV 缓存就越大，这使得部署训练基础设施比以往更加困难。Gemini、GPT 和 Claude 等模型最初在序列长度较短的情况下进行预训练，随后进行后训练以增加上下文长度。

人类通常很难在 SFT 数据中注释较长的上下文示例，因为提供高质量注释的人才资源有限。阅读冗长的文本既费时又乏味。合成数据已成为一种有用且可靠的方法，可以改善这一问题。

生成长上下文长度合成数据的一种方法是使用来自早期检查点的模型，并让其总结大段文本，并将其分块为上下文长度（目前较小）的大小。然后可以使用这些摘要（或在其他场合包括模拟问题和答案的聊天）来帮助生成一组合成数据，然后可以在 SFT 中使用。

其他示例包括生成合成数据以使诸如大海捞针基准之类的评估通过。还有许多更复杂的合成数据类型，用于训练模型概括和理解扩展上下文长度各个部分的数据。

### 强化学习

强化学习 (RL) 是指教导代理（例如大型语言模型）执行特定操作并通过最大化针对这些特定操作或实现给定结果的奖励来寻求特定结果。谈到 RL 时，需要考虑两个方面：反馈的来源以及如何整合反馈。前者是关于如何获取信号，后者是关于如何使用这些信号来更新模型。

借助强化学习，我们试图优化的大型语言模型将扮演代理的角色，它可以在给定输入或状态的情况下采取一系列操作，并根据所采取的操作获得不同的奖励。我们让代理学习可以最大化预期累积奖励的操作，从而根据强化学习目标优化此代理的行为。

有几种主要方法可以整合反馈并确定代理采取的行动 - 使用基于价值的方法或基于策略的方法，例如直接偏好优化和信任区域策略优化 (TRPO)，以及结合策略和基于价值的方法的 Actor-Critic 方法。近端策略优化 (PPO) 是 Actor-Critic 模型的一个突出示例，其更复杂的变体是所有主要 AI 实验室的主要 RL 方法。

基于价值的方法则确定到达给定状态的价值，并为每个可能状态定义值。每个状态都会根据代理从该状态开始时可以获得的预期折扣回报分配一个值，然后根据每个可用操作的价值确定其在每个步骤中的操作。从历史上看，基于价值的方法在 RL 中更常用，但现代应用程序更适合使用基于策略的方法。

在基于策略的方法中，代理由策略函数驱动，该策略函数确定在给定状态下可以采取的一组操作，并为这些操作分配概率分布。在给定状态下要执行的操作可以是确定性的，这意味着处于每个状态将始终导致相同的操作，也可以是随机的，其中概率分布描述了该给定状态下的潜在操作。然后，策略函数经过训练，以指导代理采取最大化预期回报的操作。

在强化学习期间采用基于策略的方法时，模型可以评估给定任务的最终结果来确定奖励（**结果奖励模型**( **ORM** )），也可以通过评估给定流程中的每个步骤来确定奖励（**过程奖励模型**( **PRM** )）。在训练推理模型时，使用 PRM 特别有用，因为 ORM 可以检测到推理链导致错误答案，而 PRM 可以告诉您推理链中的哪个步骤有错误。

因为策略函数指导代理在任何给定步骤中的行为——它也是一个特别有用的框架，用于优化代理/模型在推理过程的中间步骤中的行为。

结果奖励模型和过程奖励模型通常用于近端策略优化 (PPO)，这是一种常用于强化学习的算法，它迭代改进策略模型以最大化累积奖励并针对给定目标优化 LLM。在训练多步推理模型时，将 ORM 和 PRM 与 PPO 结合使用尤为重要，而多步推理模型目前是社区关注的重点。我们将在下面介绍如何为 o1 Pro 实现这一点。

## 近端策略优化（PPO）

近端策略优化（PPO）可用于对齐和微调，但它更适合在对齐期间使用的强化学习中，并且在对齐期间使用得更频繁。

对于 PPO 来说，策略指的是前面提到的使用策略模型来指示代理或模型的行为，近端指的是算法仅逐渐更新策略的方法，优化指的是通过从奖励模型提供反馈来改进策略模型，从而优化预期累积奖励的过程，从而迭代改进策略。 

上面我们主要讨论了基于策略的方法，但 PPO 在实现中同时结合了基于策略的方法和基于价值的方法。因此，可以说 PPO 使用了 Actor Critic 方法。Actor 由基于策略的模型驱动，该模型确定针对给定状态采取哪些操作（即基于策略的方法），并且有一个 Critic 根据价值函数评估所采取的操作（基于价值的方法）。因此，Actor 和 Critic 以迭代方式协同工作。

因此，最大化 PPO 目标函数将推动策略朝着有利于与优势函数的更高值相对应的动作的方向发展。
