# llama3 Post-Training

https://mp.weixin.qq.com/s/YdaTUtBe4k1FaeX1_7CXFQ

Llama3通过多轮后训练，在**人工标注或合成的数据集逐步提升模型性能。**每轮后训练包括了有监督微调（SFT）与直接偏好优化（DPO；Rafailov et al., 2024）

**1.首先，在预训练检查点的基础上，利用人类标注的偏好数据来训练一个奖励模型；**

**2.接着，采用监督微调（SFT）对预训练检查点进行精细调整；**

**3.最后，通过直接偏好优化（DPO）使SFT检查点与偏好数据对齐；**

**奖励模型的角色：**在llama3后训练框架中，奖励模型的使用方式与ChatGPT有所不同，llama3框架中的奖励模型主要用于“拒绝采样”（rejection sampling），而不是直接用于RLHF**。**拒绝采样是一种数据处理技术，它涉及从数据集中剔除掉那些被认为是低质量或不符合特定标准的样本，以此来提高整个数据集的质量和一致性**。**

**聊天对话格式：**为了调整大型语言模型（LLMs）以适用于人机交互，我们需要为模型定义一个聊天对话协议，以便其能够理解人类指令并执行对话任务。**与前代相比，Llama 3 引入了诸如工具使用等新功能，这些功能可能需要在单个对话回合内生成多条消息并发送至不同接收方（如用户端、ipython等）。**为支持这一需求，我们设计了一种新的多消息聊天协议，该协议采用多种特殊头部和终止标记。**头部标记用于指示对话中每条消息的来源和目的地。类似地，终止标记则用于指示人机交替发言的时机。**

**奖励模型：**我们在预训练检查点的基础上，训练了一个涵盖不同能力的奖励模型（RM）。**遵循Llama 2的做法，我们在过滤掉具有相似响应的样本后，使用所有偏好数据进行奖励建模。**除了标准的（选中，拒绝）响应偏好对之外，注释还针对某些提示创建了第三个“编辑后的响应”，其中对选中的响应进行了进一步编辑以进行优化。因此，**每个偏好排序样本都有两个或三个响应，并带有明确的排序（编辑后 > 选中 > 拒绝）**。**在训练过程中，我们将提示和多个响应连接成单行，并随机打乱响应的顺序。**这种做法近似于将响应放在单独的行中并计算分数的标准场景，但在我们的消融实验中，这种方法提高了训练效率，同时没有损失准确性。

**有监督微调：**随后，我们使用奖励模型对我们的人类标注提示进行拒绝采样。**结合这种拒绝采样数据和其他数据源（包括合成数据），我们使用目标标记上的标准交叉熵损失（同时对提示标记进行损失屏蔽）对预训练语言模型进行微调**。尽管许多训练目标是由模型生成的，但我们仍将这一阶段称为监督微调（SFT；Wei等人，2022a；Sanh等人，2022；Wang等人，2022b）。

**直接偏好优化：**为了让模型更好地与人类偏好保持一致，我们采用直接偏好优化（DPO；Rafailov等人，2024年）技术来训练我们的SFT模型**。在训练过程中，**我们主要使用前几轮对齐中表现最佳模型所收集的最新一批偏好数据。这样，我们的训练数据就能更好地符合每轮优化中策略模型的分布。我们还探索了诸如PPO（Schulman等人，2017年）之类的在线策略算法，但发现DPO在大型模型上所需的计算量更少，且表现更佳，特别是在像IFEval（Zhou等人，2023年）这样的指令遵循基准测试中。**

**对于Llama 3，我们设置了1e-5的学习率，并将β超参数设置为0.1，此外，我们还对DPO算法进行了以下修改：**

**在DPO损失中屏蔽格式化标记：为了稳定DPO训练，我们从选中和拒绝响应的损失中屏蔽了包括头部和终止标记在内的特殊格式化标记。我们发现，如果这些标记对损失有所贡献，可能会导致模型出现不期望的行为，如尾部重复或突然生成终止标记。我们假设这是由于DPO损失的对比性质所致——选中和拒绝响应中共同存在的标记导致了一个相互冲突的学习目标，因为模型需要同时增加和减少这些标记的可能性。**

**使用NLL损失进行正则化：我们添加了一个额外的负对数似然（NLL）损失项，其缩放系数为0.2，专门针对选中序列，类似于Pang等人（2024年）的做法。这有助于通过保持生成所需的格式化并防止选中响应的对数概率降低来进一步稳定DPO训练（Pang等人，2024年；Pal等人，2024年）。**
