# SmolLM2: When Smol Goes Big — Data-Centric Training of a Small Language Model

摘要：

虽然大语言模型在人工智能的许多应用领域中推动了重大突破，但因其本身规模庞大，导致计算成本高昂，并且在资源受限的环境中部署起来颇具挑战。在本文中，我们记录了SmolLM2的研发过程，这是一款处于前沿水平的 “小型”（17亿参数）语言模型。为了实现出色的性能，我们使用多阶段训练过程，在约11万亿个数据标记上对SmolLM2进行了过度训练，该过程将网络文本与专业的数学、代码以及指令跟随数据相混合。此外，在发现现有数据集存在规模过小或质量不高的问题的阶段，我们引入了新的专业数据集（精细数学数据集FineMath、教育堆栈数据集Stack-Edu和SmolTalk数据集）。为了指导我们的设计决策，我们既进行了小规模的对比实验，也进行了手动优化过程，根据前一阶段的性能表现，更新每个阶段的数据集混合比例。最终，我们证明了SmolLM2的性能优于其他近期的小型语言模型，其中包括通义千问2.5（Qwen2.5）的15亿参数版本以及Llama3.2的10亿参数版本。为了推动未来在语言模型开发以及小型语言模型应用方面的研究，我们不仅发布了SmolLM2，还发布了我们在该项目过程中准备的所有数据集。

---

简介：

语言模型（LM）的性能和表现的一个关键因素在于用于训练它们的数据。虽然数据筛选对于任何规模的语言模型都很重要，但对于较小规模的模型而言，其影响尤为显著。因为较小模型的能力有限，必须精心优化这些能力，以便让模型学习核心知识和基本能力，而非仅仅记住一些偶然的事实。

预训练数据集的构建：选择和混合

此外，设计预训练策略不仅涉及到数据的选择和筛选，还包括确定从不同来源 “混合”（即采样）的数据量，这在纳入例如专业数学和代码数据集时尤为重要。因此，我们对现有的数据集进行了仔细评估，并且在我们认为有必要的情况下，创建了新的、经过改进且规模更大的数据集。

英文网络数据：

FineWebEdu consists of 1.3T tokens that were deemed “educational” by a classifier trained on annotations generated by Llama3-70B-Instruct

DCLM comprises 3.8T tokens filtered using a fastText classifier (Joulin et al., 2016a;b) trained on instruction-following data from OpenHermes 2.5 (Teknium, 2023a) and high-scoring posts from the r/ExplainLikeImFive (ELI5) subreddit.

我们发现，**FineWeb-Edu在教育基准测试MMLU、ARC和OpenBookQA上取得了更高的分数，而DCLM在HellaSwag和常识问答（CommonsenseQA）测试中表现更佳。**这些结果与数据集的内容相符：FineWeb-Edu优先收录教育类材料，而DCLM则涵盖了更多样化的对话风格内容。

**60% FineWeb-Edu and 40% DCLM mix works well**

math数据：专门的数学预训练数据对于培养可靠的数学理解能力至关重要。近期的研究表明，从通用网络数据抓取（Common Crawl）中精心筛选出的数学内容，再结合有针对性的筛选技术，能够显著提升语言模型的数学推理能力。

* OpenWebMath：consists of 12B tokens, built by filtering math-specific content from Common Crawl and using a specialized text extraction pipeline to preserve mathematical formatting and equations.
* InfiMMWebMath：contains 40B text tokens, and its authors show that it matches the performance of the private dataset of DeepSeekMath.

消融结论：

InfiMM-WebMath achieves a peak accuracy of 14% on GSM8K compared to OWM’s 10%, while OWM slightly outperforms InfiMM-WebMath on MATH.

进一步的分析突出了两个关键的局限性：数据集规模不足，对逐步的数学推理关注不够，同时聚焦于高深概念的学术论文在数据集中占比过高。

新数据集：FINEMATH

通过基于分类器的筛选，收集了多达 540 亿个专注于数学演绎和推理的数学数据标记。

从Common Crawl中选取了5.8B的页面，然后使用基于分类器的方法，从中筛选出包含高质量数学内容的页面。

We then employed the FineWeb-Edu filtering approach, using Llama-3.1-70B-Instruct (Dubey et al., 2024) with a prompt (Appendix C.2) that scores content on a 3-point scale.

在根据这些 “伪标签” 对分类器进行训练后，我们确定了那些包含至少10个质量得分达到2分或更高页面的领域。我们通过纳入至少有10个来自开放网络数学（OWM）或无限数学网络版（InfiMM-WebMath）的统一资源定位符（URL）的领域，扩大了我们的领域覆盖范围。从通用网络数据抓取（Common Crawl）索引中，我们总共检索到了属于这个领域列表的77亿个URL：其中57亿个是由我们的分类器识别出来的，6亿个来自OWM，13亿个来自InfiWebMath。然后，我们使用OWM的处理流程重新提取了所有已识别的页面，保留了LaTeX格式，并删除了所有的样板页面，最终得到了包含65万亿个标记的71亿个页面。

为了仅保留高质量的数学内容，我们再次运用了一个分类器，该分类器是依据Llama-3.1-70B-Instruct的注释进行训练的，使用了一个5分制的提示（附录C.3），专门针对包含推理内容以及初高中水平内容的页面。我们注意到，InfiMM-WebMath采用了类似的分类器筛选流程，但他们的提示所针对的内容类型有所不同。在分类之后，我们使用了具有10个哈希值的单波段最小哈希局部敏感哈希算法（MinHash LSH，布罗德，1997年）进行去重处理，并应用了快速文本（fastText）语言分类方法（茹兰等人，2016a；2016b），以仅保留英语内容。

最终，我们开发了多个版本的精细数学数据集（FineMath），其中包括FineMath4+（100亿个标记，670万份文档），它只保留了得分在4到5分之间的样本，以及FineMath3+（340亿个标记，2140万份文档），该版本纳入了得分在3到5分之间的样本。此外，我们将相同的分类器应用于InfiMMWebMath数据集，创建了Infi-WebMath4+（85亿个标记，630万份文档）和Infi-WebMath3+（205亿个标记，1390万份文档）。与杨等人（2024c）的做法类似，我们使用13-gram匹配法，并设定与最长公共子序列的最小重叠率为0.6，对每个数据集进行处理，以排除其中与GSM8K、MATH和MMLU-STEM数据集重复的内容。  结果 图1展示了我们对FineMath数据集进行的退火消融实验结果。在GSM8K、MATH和MMLU-STEM测试中，所有FineMath子数据集的表现都始终优于开放网络数学数据集（OWM）和InfiMM-WebMath数据集。与InfiMM-WebMath数据集相比，FineMath4+在GSM8K测试中的成绩提升了一倍，在MATH测试中的成绩提升了六倍，这表明了保留包含推理过程的高质量数学内容的重要性。此外，Infi-WebMath4+的表现优于InfiMM-WebMath，但在处理800亿个标记（大约10个训练周期）后性能趋于平稳，这很可能是由于数据重复导致的，而这种趋势在FineMath4+中并未出现。

---

代码数据：

that including code data in pretraining enhances not only code-related capabilities but also improves natural language reasoning and world knowledge

Stack-Edu：

最近的研究表明，基于分类器筛选的FineWeb-Edu策略对代码数据同样有效（魏等人，2024b；阿拉尔等人，2024）。因此，我们构建了教育堆栈数据集（Stack-Edu），这是StarCoder2Data数据集的一个经过筛选的版本，重点关注具有教育意义且文档记录完善的代码。具体来说，我们从StarCoder2Data数据集中选取了15种规模最大的编程语言，以适应较小规模模型的能力限制（洛日科夫等人，2024），并确保在消融实验中有足够的基准测试覆盖范围。这个子数据集大约包含4500亿个标记。**然后，我们使用StarEncoder模型（李等人，2023a），基于Llama3-70B-Instruct模型（杜贝等人，2024）生成的人工合成注释（提示内容见附录D.1），训练了15个特定语言的分类器。这些注释以0到5分的评分标准对代码的教育质量进行评估。每个分类器都在50万个样本上进行了训练，并且在将二分类阈值设为3时，对于大多数语言，其F1分数都达到了0.7以上。**

---

预训练：

在构建SmolLM2时，我们在11万亿个标记上进行了训练（在我们收集的数据集上大约训练了两个周期），采用了多阶段训练方法，而不是在整个预训练过程中使用固定的数据集混合方式。这一设计遵循了四项关键原则：

（1）**以性能为导向进行干预**，即我们会监控关键基准测试的评估指标，并调整数据集的混合比例，以解决特定的能力瓶颈问题；

（2）**在退火阶段对高质量的数学和代码数据进行过采样**，将诸如FineMath数据集以及部分Stack-Edu数据集保留到最后阶段使用，以最大限度地发挥它们的作用（布莱肯尼等人，2024；艾伦人工智能研究所（Ai2），2024）；

（3）**在训练中期有策略地引入中等规模的数据集**，如开放网络数学数据集（OWM）、InfiMM-WebMath数据集和Stack-Edu数据集，以避免在早期被规模较大的数据集所稀释；

（4）**避免数据过度重复**，根据明尼霍夫等人（2023）的建议，我们力求使大多数数据集的训练周期接近推荐的4至5个周期的阈值。虽然进行多次从头开始的训练来探索不同的数据混合计划可能会有所收获，但预训练SmolLM2的成本高昂（大约需要25万美元的GPU计算成本），这促使我们采用了 “在线” 训练方法。


在第二阶段训练之后，大多数语言的代码处理性能都有所提升，这验证了对StarCoderData数据集进行过采样这一决策的正确性。开放网络数学数据集（OWM）的融入对数学性能没有产生显著影响，这凸显了在后续阶段需要使用规模更大、质量更高的数学数据集的必要性。除了代码和数学性能之外，如图6（附录E.2）所示，我们在采用多项选择形式（MCF，即明确输出 “A”、“B”、“C” 或 “D” 中的一个选项，而不是像完形填空形式那样计算不同答案的可能性）的情况下，观察到了常识性机器学习基准测试（MMLU）的准确率高于随机水平（超过25%）。这与之前的研究结果形成了对比，之前的研究表明小型模型在处理多项选择形式的测试时存在困难（顾等人，2024；杜等人，2024），这表明对小型模型进行长时间的训练能够使其获得通常与大型模型相关的能力（布莱肯尼等人，2024；顾等人，2024；杜等人，2024）。为了进一步优化MMLU的性能，我们重新审视了英语数据集的混合方式，并进行了额外的退火消融实验，结果发现，在这个阶段，相对于FineWeb-Edu数据集增加对话语言模型数据集（DCLM）的比例，能够略微提高MMLU多项选择形式测试的准确率。

尽管这些新数据集的融入在多个基准测试中都带来了性能提升，但我们注意到，在这个阶段出现了明显的性能损失激增现象，即便回滚训练过程并跳过与激增相关的数据后，这种现象依然存在（乔杜里等人，2023；阿尔马兹鲁伊等人，2023）。确切原因尚未确定，但到该阶段结束时，大多数评估指标都恢复了。

虽然所有的基准测试任务在第四阶段之后都有了性能提升，但我们注意到，在编码性能方面有了显著进步，最值得一提的是，数学性能也大幅提高，这验证了我们专门针对这些领域设计的数据混合策略是有效的。

---

后训练：

For post-training, we leveraged existing datasets in addition to a new instruction tuning dataset called SmolTalk. 一个全新的指令跟随数据集，它精心地将经过挑选的现有数据集与我们开发的新合成数据集相结合，其中包括喜鹊超大型对话数据集（Magpie-Ultra conversational dataset），以及其他针对特定能力的专业数据集，如Smol-Constraint（斯莫尔约束数据集）、Smol-Rewrite（斯莫尔改写数据集）和Smol-Summarization（斯莫尔摘要数据集）。**所有这些数据集都是使用Distilabel工具生成的。**

喜鹊超大型对话数据集（MagPieUltra）利用了规模更大、功能更强的模型Llama-3.1-405B-Instruct-FP8（杜贝等人，2024）。我们还纳入了系统提示来引导数据生成，从而产生了一个包含100万个样本的平衡数据集，其中的对话均为三个轮次。生成的数据集进一步使用较小的Llama模型（Llama-3.1-8B-Instruct和Llama-Guard-3-8B）进行筛选，以确保所生成指令的质量和安全性。我们还利用了ArmoRM（王等人，2024b；2024a）来对对话进行评分，以便基于质量进行筛选，并使用gte-large-env1.5（张等人，2024；李等人，2023c）对语义相似的对话进行去重处理。

Our dataset outperforms MagPie-Pro on most benchmarks, and largely surpasses OpenHermes2.5 and UltraChat (Ding et al., 2023) on IFEval and MT-Bench.

**TASK-SPECIFIC DATA**

对于总结数据集和改写数据集，我们首先生成了高质量的源文本，这些文本将作为总结和改写任务的基础。我们使用 PersonaHub（葛等人，2024）以及来自 FinePersonas 数据集（阿吉拉，2024；陈等人，2024）的人物角色，合成了一系列多样的电子邮件、推文、领英帖子和笔记。通过使用特定的系统提示和人物角色描述来提示通义千问 2.5 的 720 亿参数指令模型，我们能够生成多样的内容，获得具有各种写作风格、主题和视角的文本。然后，我们提示通义千问 2.5 的 720 亿参数指令模型对给定的文本进行总结和改写，得到了大约 100 万条总结文本和 60 万条改写后的文本。

如表 10（附录 F）所示，将这三个斯莫尔数据集添加到喜鹊超大型对话数据集（MagPie-Ultra）中（即 MagPieUltra+），进一步提升了在 IFEval 测试中的性能。

数学数据：

为了提高数学推理能力，我们通过在由**80%的通用指令数据**（喜鹊超大型对话数据集（MagPie Ultra）加上斯莫尔约束数据集（Smol-Constraint）、斯莫尔改写数据集（Smol-Rewrite）、斯莫尔总结数据集（Smol-Summarization））和**20%来自不同来源的数学数据**组成的混合数据上进行微调，对公开的数学指令数据集进行了评估。附录F中的表10所示的结果突显了不同数据集的互补优势：努米纳数学思维链数据集（NuminaMath-CoT，李等人，2024b）在MATH和MT-Bench测试中表现出色，而同样包含在OpenHermes2.5中的元数学问答数据集（MetaMathQA，于等人，2023）则提高了在GSM8K测试中的成绩。基于这些发现，我们将这两个数据集结合起来，纳入到了斯莫尔对话数据集（SmolTalk）中。

OTHER SPECIALIZED DATA

对于代码生成任务，我们使用了 “自开源软件-星编码器2指令数据集（Self-OSS-Starcoder2-Instruct，魏等人，2024a）”，**该数据集包含5万个高质量的Python指令-回复对**。为了支持系统提示，我们从 “系统对话2.0（SystemChats2.0，计算公司，2024）” 中随机选取了3万个样本纳入其中；而对于函数调用任务，我们添加了来自 “APIGen-函数调用数据集（**APIGen-FunctionCalling**，刘等人，2024）” 的8万个样本。此外，为了在长上下文任务中保持良好的性能，我们纳入了 “长对齐数据集（**LongAlign**，白等人，2024）” 的一个英语子集（包含3700个样本，每个样本有8000至16000个标记）。由于 “OpenHermes2.5” 在知识测试（MMLU-Pro）、日常对话测试（Face，2024）、2200个休闲多轮交互以及 “探索指令数据集（ExploreInstruct，万等人，2023）” 的改写任务中表现出色，**我们还随机选取了10万个OpenHermes2.5的样本添加进来**。我们发现，将这些包含特定数量样本的数据集整合进来，**有效地增强了模型在目标能力方面的表现，同时在其他基准测试中也保持了良好的性能。**

---

Alignment：

对于偏好学习，我们采用了直接偏好优化（DPO，拉法伊洛夫等人，2024）方法。我们对各种公开的合成反馈数据集（伊维森等人，2024）进行了试验，其中包括超反馈数据集（UltraFeedback，崔等人，2024）、超交互数据集（UltraInteract，袁等人，2024）、水豚数据集（Capybara，达尼埃莱和素帕瓦迪普拉西特，2023）以及虎鲸数据集（ORCA，吕等人，2023）。事实证明，超反馈数据集在各个基准测试中最为稳定有效，提升了MT-Bench、MMLU-Pro和MATH测试的成绩。我们以1.0×10⁻⁶的学习率、0.5的贝塔值、128的全局批量大小以及1024个标记的序列长度进行了2个周期的训练。在完成这一DPO训练的最后阶段后，我们得到了指令型SmolLM2模型。正如杜贝等人（2024）所指出的那样，在DPO中使用短上下文数据并未影响模型处理8000个标记上下文的能力。

---
